{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPgmg9CQ6oKOCL8SHH2qqA+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"WhnvjzFCofNG"},"outputs":[],"source":["import torch\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"code","source":["# 구글 코랩에서 데이터셋 다운로드 및 준비\n","from google.colab import files\n","files.upload()  # 'kaggle.json' 파일 업로드\n","\n","!mkdir -p ~/.kaggle\n","!cp kaggle.json ~/.kaggle/\n","!chmod 600 ~/.kaggle/kaggle.json\n","\n","# 아몬드 데이터셋 다운로드\n","!kaggle datasets download -d mahyeks/almond-varieties\n","\n","# almond 폴더에 데이터 압축 해제\n","!unzip almond-varieties.zip -d almond"],"metadata":{"id":"mRl1ksg0pq5N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# EDA\n","\n","import os\n","import cv2\n","import random\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from collections import Counter\n","\n","# 데이터 폴더 경로\n","root_dir = 'almond/dataset'\n","\n","# 라벨 목록\n","labels = os.listdir(root_dir)\n","\n","# 각 라벨별 데이터 개수 확인\n","data_counts = {}\n","example_images = {}\n","image_sizes = []\n","aspect_ratios = []\n","color_distributions = []\n","brightness_values = []\n","file_types = Counter()\n","\n","for label in labels:\n","    data_dir = os.path.join(root_dir, label)\n","    datas = os.listdir(data_dir)\n","    data_counts[label] = len(datas)\n","\n","    # 예시 이미지 저장\n","    if datas:\n","        example_images[label] = os.path.join(data_dir, random.choice(datas))\n","\n","    # 이미지 크기 및 색상 분석\n","    for img_name in datas:\n","        img_path = os.path.join(data_dir, img_name)\n","        file_extension = os.path.splitext(img_name)[1].lower()\n","        file_types[file_extension] += 1\n","\n","        img = cv2.imread(img_path)\n","        if img is not None:\n","            h, w, _ = img.shape\n","            image_sizes.append((w, h))\n","            aspect_ratios.append(w / h)\n","\n","# 데이터 분포 시각화\n","plt.figure(figsize=(8, 5))\n","plt.bar(data_counts.keys(), data_counts.values(), color='skyblue')\n","plt.xlabel('Labels')\n","plt.ylabel('Number of Images')\n","plt.title('Dataset Distribution')\n","plt.show()\n","\n","# 예시 이미지 시각화\n","fig, axes = plt.subplots(1, len(example_images), figsize=(15, 5))\n","\n","for ax, (label, img_path) in zip(axes, example_images.items()):\n","    img = cv2.imread(img_path)\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    ax.imshow(img)\n","    ax.set_title(label)\n","    ax.axis('off')\n","\n","plt.show()\n","\n","# 데이터 개수 출력\n","print(\"Dataset Image Count:\")\n","for label, count in data_counts.items():\n","    print(f\"{label}: {count} images\")\n","\n","# 최소, 최대 이미지 크기 출력\n","if image_sizes:\n","    min_size = min(image_sizes, key=lambda x: x[0] * x[1])\n","    max_size = max(image_sizes, key=lambda x: x[0] * x[1])\n","    print(f\"Smallest Image Size: {min_size[1]}x{min_size[0]}\")\n","    print(f\"Largest Image Size: {max_size[1]}x{max_size[0]}\")\n","\n","# 파일 형식 개수 출력\n","print(\"File Type Distribution:\")\n","for ext, count in file_types.items():\n","    print(f\"{ext}: {count} files\")\n"],"metadata":{"id":"J3ePpIF8psLF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**초기 실험**"],"metadata":{"id":"pvPXVbDVuyiK"}},{"cell_type":"code","source":["# data 비율 별 dataloader\n","\n","import torch\n","\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader, random_split\n","\n","def set_dataloader(split_ratio):\n","    # 라벨 목록\n","    labels = os.listdir(root_dir)\n","    label_to_index = {label: idx for idx, label in enumerate(labels)}\n","    num_classes = len(labels)\n","\n","    # 전체 데이터 리스트 생성 (폴더별로 일정 비율 유지)\n","    data_list = {label: [] for label in labels}\n","    for label in labels:\n","        data_dir = os.path.join(root_dir, label)\n","        datas = os.listdir(data_dir)\n","        for img_name in datas:\n","            img_path = os.path.join(data_dir, img_name)\n","            data_list[label].append((img_path, label_to_index[label]))\n","\n","    # 데이터셋 분할 (폴더별로 유지)\n","    train_data, val_data, test_data = [], [], []\n","    for label, items in data_list.items():\n","        random.shuffle(items)\n","        total_size = len(items)\n","        train_end = int(split_ratio * total_size)\n","        val_end = train_end + int((split_ratio/2) * total_size)\n","\n","        train_data.extend(items[:train_end])\n","        val_data.extend(items[train_end:val_end])\n","        test_data.extend(items[val_end:])\n","\n","    # 데이터셋 클래스 정의\n","    class CustomImageDataset(Dataset):\n","        def __init__(self, data, transform=None):\n","            self.data = data\n","            self.transform = transform\n","\n","        def __len__(self):\n","            return len(self.data)\n","\n","        def __getitem__(self, idx):\n","            img_path, label = self.data[idx]\n","            img = cv2.imread(img_path)\n","            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","            if self.transform:\n","                img = self.transform(img)\n","\n","            return img, label\n","\n","    # 데이터 변환 정의\n","    transform = transforms.Compose([\n","        transforms.ToPILImage(),\n","        transforms.Resize((224, 224)),  # 이미지 크기 조정\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ])\n","\n","    # 데이터셋 생성\n","    train_dataset = CustomImageDataset(train_data, transform=transform)\n","    val_dataset = CustomImageDataset(val_data, transform=transform)\n","    test_dataset = CustomImageDataset(test_data, transform=transform)\n","\n","    batch_size = 16\n","\n","    # 데이터로더 생성\n","    dataloader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n","    dataloader_val = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n","    dataloader_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n","\n","    # 데이터셋 크기 출력\n","    print(f\"Training Dataset Size: {len(train_dataset)}\")\n","    print(f\"Validation Dataset Size: {len(val_dataset)}\")\n","    print(f\"Test Dataset Size: {len(test_dataset)}\")\n","\n","    # 데이터 확인\n","    for images, labels in dataloader_train:\n","        print(f\"Train Batch Image Shape: {images.shape}\")\n","        print(f\"Train Batch Labels: {labels}\")\n","        break\n","\n","    return dataloader_train, dataloader_val, dataloader_test, num_classes"],"metadata":{"id":"JEoI82ZpqMuf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 정의 함수\n","\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.models as models\n","\n","def get_model(model_name, num_classes):\n","    if model_name == 'resnet50':\n","        model = models.resnet50(pretrained=True)\n","        model.fc = nn.Linear(model.fc.in_features, num_classes)\n","    elif model_name == 'efficientnet_b0':\n","        model = models.efficientnet_b0(pretrained=True)\n","        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n","    elif model_name == 'mobilenet_v3_small':\n","        model = models.mobilenet_v3_small(pretrained=True)\n","        model.classifier[3] = nn.Linear(model.classifier[3].in_features, num_classes)\n","    elif model_name == 'shufflenet_v2_x1_0':\n","        model = models.shufflenet_v2_x1_0(pretrained=True)\n","        model.fc = nn.Linear(model.fc.in_features, num_classes)\n","    else:\n","        raise ValueError(\"Unsupported model\")\n","    return model"],"metadata":{"id":"RdQIFCxmq-6G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습 함수\n","def train_model(model_name, train_loader, val_loader, num_classes, save_dir, device, split_ratio, epochs=10, lr=0.001, patience=3):\n","    model = get_model(model_name, num_classes=num_classes)\n","    model = model.to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    best_val_loss = float('inf')\n","    no_improve_count = 0\n","    log_file = open(f\"{save_dir}/training_log_{model_name}.txt\", \"w\")\n","    log_file.write(f\"Model: {model_name}, Epochs: {epochs}, Learning Rate: {lr}, Patience: {patience}, Train Ratio: {split_ratio*100}%, Validation Ratio: {split_ratio*50}%\\n\")\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","\n","        for images, labels in train_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","        train_acc = correct / total\n","        val_loss, val_acc = validate_model(model, val_loader, device, criterion)\n","        log_file.write(f\"Epoch {epoch+1}/{epochs}, Train Loss: {running_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\\n\")\n","        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {running_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            no_improve_count = 0\n","            torch.save(model.state_dict(), f\"{save_dir}/best_model_{model_name}.pth\")\n","        else:\n","            no_improve_count += 1\n","\n","        if no_improve_count >= patience:\n","            print(f\"Early stopping at epoch {epoch+1} due to no improvement in validation accuracy.\")\n","            log_file.write(f\"Early stopping at epoch {epoch+1}\\n\")\n","            break\n","\n","    log_file.close()\n","    print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n","\n","# validation 평가 함수 (Loss와 Accuracy 반환)\n","def validate_model(model, dataloader, device, criterion):\n","    model.eval()\n","    total_loss = 0.0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for images, labels in dataloader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item()\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    avg_loss = total_loss / len(dataloader)\n","    accuracy = correct / total\n","    return avg_loss, accuracy"],"metadata":{"id":"H8zqwu0UrA8Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_next_directory(base_path):\n","    # base_path 경로 내의 기존 디렉토리 목록 확인\n","    existing_dirs = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]\n","\n","    # 숫자로 된 디렉토리만 필터링\n","    numbered_dirs = sorted([int(d) for d in existing_dirs if d.isdigit()])\n","\n","    # 다음 디렉토리 번호 결정\n","    next_number = numbered_dirs[-1] + 1 if numbered_dirs else 1\n","    new_dir = os.path.join(base_path, str(next_number))\n","\n","    # 디렉토리 생성\n","    os.makedirs(new_dir, exist_ok=True)\n","    print(f\"Created directory: {new_dir}\")\n","\n","    return new_dir"],"metadata":{"id":"9NKRpZARrF0n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 평가 함수\n","import seaborn as sns\n","\n","from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n","\n","# Fine-tuned 모델 불러오는 함수\n","def load_fine_tuned_model(model_name, num_classes, checkpoint_path, device):\n","    if model_name == 'resnet50':\n","        model = models.resnet50(pretrained=False)\n","        model.fc = nn.Linear(model.fc.in_features, num_classes)\n","    elif model_name == 'efficientnet_b0':\n","        model = models.efficientnet_b0(pretrained=False)\n","        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n","    elif model_name == 'mobilenet_v3_small':\n","        model = models.mobilenet_v3_small(pretrained=True)\n","        model.classifier[3] = nn.Linear(model.classifier[3].in_features, num_classes)\n","    elif model_name == 'shufflenet_v2_x1_0':\n","        model = models.shufflenet_v2_x1_0(pretrained=True)\n","        model.fc = nn.Linear(model.fc.in_features, num_classes)\n","    else:\n","        raise ValueError(\"Unsupported model\")\n","\n","    model.load_state_dict(torch.load(checkpoint_path))\n","    model.to(device)\n","    return model\n","\n","\n","# Confusion Matrix 시각화 함수\n","def plot_confusion_matrix(y_true, y_pred, classes, model_name):\n","    cm = confusion_matrix(y_true, y_pred)\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n","    plt.xlabel('Predicted Label')\n","    plt.ylabel('True Label')\n","    plt.title(f'{model_name} Confusion Matrix')\n","    plt.show()\n","\n","# 모델 성능 평가 및 Confusion Matrix 출력\n","def evaluate_model(model_name, dir_num, dataloader, device):\n","    checkpoint_path = os.path.join(dir_num, f\"best_model_{model_name}.pth\")\n","    model = load_fine_tuned_model(model_name, num_classes, checkpoint_path, device)\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","    with torch.no_grad():\n","        for images, labels in dataloader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, preds = torch.max(outputs, 1)\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    # 데이터 폴더 경로\n","    root_dir = 'almond/dataset'\n","\n","    # 라벨 목록\n","    labels = os.listdir(root_dir)\n","\n","    # 평가 지표 계산\n","    accuracy = accuracy_score(all_labels, all_preds)\n","    precision = precision_score(all_labels, all_preds, average='weighted')\n","    recall = recall_score(all_labels, all_preds, average='weighted')\n","    f1 = f1_score(all_labels, all_preds, average='weighted')\n","\n","    print(f\"Test Accuracy: {accuracy:.4f}\")\n","    print(f\"Test Precision: {precision:.4f}\")\n","    print(f\"Test Recall: {recall:.4f}\")\n","    print(f\"Test F1 Score: {f1:.4f}\")\n","\n","    plot_confusion_matrix(all_labels, all_preds, labels, model_name)\n","\n","# 평가 함수\n","import os\n","import torch\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","from torch import nn\n","from torchvision import models\n","from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n","\n","def process_single_experiment(dataloader, device, num_classes, experiment_dir):\n","    \"\"\"\n","    특정 실험 폴더(예: /content/drive/MyDrive/KTB/personal_mission2/train_dir/1)에서만\n","    학습 로그 및 Confusion Matrix를 저장하는 함수\n","    \"\"\"\n","\n","    # train_dir에서 test_dir로 변환\n","    test_experiment_dir = experiment_dir.replace(\"train_dir\", \"test_dir\")\n","\n","    # experiment_dir 존재 여부 확인\n","    if not os.path.exists(experiment_dir):\n","        print(f\"⚠️ 유효하지 않은 실험 디렉토리: {experiment_dir}\")\n","        return\n","\n","    # 해당 실험 폴더 안의 training_log 파일 찾기\n","    txt_files = [os.path.join(experiment_dir, file) for file in os.listdir(experiment_dir) if file.startswith(\"training_log_\") and file.endswith(\".txt\")]\n","\n","    if not txt_files:\n","        print(f\"❌ {experiment_dir}에서 training_log 파일을 찾을 수 없습니다.\")\n","        return\n","\n","    os.makedirs(test_experiment_dir, exist_ok=True)  # test_dir에 해당 실험 폴더 생성\n","\n","    for txt_file in txt_files:\n","        df = parse_log(txt_file)\n","        if df is not None:\n","            model_name = extract_model_name(os.path.basename(txt_file))\n","\n","            # 학습 로그 그래프 저장\n","            plt.figure(figsize=(12, 5))\n","\n","            # Loss 그래프\n","            plt.subplot(1, 2, 1)\n","            plt.plot(df[\"Epoch\"], df[\"Train Loss\"], label=\"Train Loss\", marker=\"o\")\n","            plt.plot(df[\"Epoch\"], df[\"Val Loss\"], label=\"Val Loss\", marker=\"s\")\n","            plt.xlabel(\"Epoch\")\n","            plt.ylabel(\"Loss\")\n","            plt.title(f\"Training & Validation Loss ({model_name})\")\n","            plt.legend()\n","            plt.grid(True)\n","\n","            # Accuracy 그래프\n","            plt.subplot(1, 2, 2)\n","            plt.plot(df[\"Epoch\"], df[\"Train Acc\"], label=\"Train Acc\", marker=\"o\")\n","            plt.plot(df[\"Epoch\"], df[\"Val Acc\"], label=\"Val Acc\", marker=\"s\")\n","            plt.xlabel(\"Epoch\")\n","            plt.ylabel(\"Accuracy\")\n","            plt.title(f\"Training & Validation Accuracy ({model_name})\")\n","            plt.legend()\n","            plt.grid(True)\n","\n","            plt.tight_layout()\n","            plt.savefig(os.path.join(test_experiment_dir, f\"training_log_{model_name}.png\"))\n","            plt.close()\n","\n","            # Confusion Matrix 및 성능 지표 저장\n","            evaluate_model(model_name, experiment_dir, dataloader, device, num_classes)\n","\n","    print(f\"✅ {experiment_dir}의 학습 로그 및 Confusion Matrix가 {test_experiment_dir}에 저장되었습니다.\")\n","\n"],"metadata":{"id":"DzfHpvNArHl8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for ratio in [0.8, 0.7, 0.6, 0.5, 0.4]:\n","    dataloader_train, dataloader_val, dataloader_test, num_classes = set_dataloader(ratio)\n","\n","    save_dir = create_next_directory('/content/drive/MyDrive/KTB/personal_mission/train_dir')\n","    for model_name in ['resnet50', 'efficientnet_b0', 'mobilenet_v3_small', 'shufflenet_v2_x1_0']:\n","        print(f\"Training {model_name}...\")\n","        train_model(model_name, dataloader_train, dataloader_val, num_classes, save_dir, device, ratio, epochs=100, lr=0.001, patience=10)\n","    process_single_experiment(dataloader_test, device, num_classes, save_dir)"],"metadata":{"id":"FLDo4IjnsJ6b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for ratio in [0.3, 0.2, 0.1, 0.08, 0.06, 0.04, 0.03, 0.02, 0.01]:\n","    dataloader_train, dataloader_val, dataloader_test, num_classes = set_dataloader(ratio)\n","\n","    save_dir = create_next_directory('/content/drive/MyDrive/KTB/personal_mission/train_dir')\n","    for model_name in ['resnet50', 'efficientnet_b0', 'mobilenet_v3_small', 'shufflenet_v2_x1_0']:\n","        print(f\"Training {model_name}...\")\n","        train_model(model_name, dataloader_train, dataloader_val, num_classes, save_dir, device, ratio, epochs=100, lr=0.0001, patience=10)\n","    process_single_experiment(dataloader_test, device, num_classes, save_dir)"],"metadata":{"id":"SQidgP6_rjzU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 성능 평가 및 Confusion Matrix 저장\n","def evaluate_baseline_model(model_name, dir_path, dataloader, device, num_classes):\n","    model = get_model(model_name, num_classes)\n","    model.to(device)\n","    if model != None:\n","        model.eval()\n","        all_preds, all_labels = [], []\n","\n","        with torch.no_grad():\n","            for images, labels in dataloader:\n","                images, labels = images.to(device), labels.to(device)\n","                outputs = model(images)\n","                _, preds = torch.max(outputs, 1)\n","                all_preds.extend(preds.cpu().numpy())\n","                all_labels.extend(labels.cpu().numpy())\n","\n","        # 데이터셋의 클래스 리스트 가져오기\n","        root_dir = 'almond/dataset'\n","        classes = os.listdir(root_dir)\n","\n","        # 평가 지표 계산\n","        accuracy = accuracy_score(all_labels, all_preds)\n","        precision = precision_score(all_labels, all_preds, average='weighted')\n","        recall = recall_score(all_labels, all_preds, average='weighted')\n","        f1 = f1_score(all_labels, all_preds, average='weighted')\n","\n","        print(f\"Test Accuracy: {accuracy:.4f}\")\n","        print(f\"Test Precision: {precision:.4f}\")\n","        print(f\"Test Recall: {recall:.4f}\")\n","        print(f\"Test F1 Score: {f1:.4f}\")\n","\n","        # 저장 경로 변환 (train_dir -> test_dir)\n","        save_path = dir_path.replace(train_root, test_root)\n","        os.makedirs(save_path, exist_ok=True)\n","        cm_save_path = os.path.join(save_path, f\"confusion_matrix_{model_name}.png\")\n","\n","        plot_confusion_matrix(all_labels, all_preds, classes, model_name, cm_save_path)\n","\n","        # 평가지표 저장\n","        metrics_save_path = os.path.join(save_path, f\"metrics_{model_name}.txt\")\n","        with open(metrics_save_path, \"w\") as f:\n","            f.write(f\"Model: {model_name}\\n\")\n","            f.write(f\"Test Accuracy: {accuracy:.4f}\\n\")\n","            f.write(f\"Test Precision: {precision:.4f}\\n\")\n","            f.write(f\"Test Recall: {recall:.4f}\\n\")\n","            f.write(f\"Test F1 Score: {f1:.4f}\\n\")\n","\n","        print(f\"📄 {metrics_save_path}에 평가지표 저장 완료!\")"],"metadata":{"id":"O4gKyqbpuAG1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for model_name, ratio in zip(['resnet50', 'efficientnet_b0', 'mobilenet_v3_small', 'shufflenet_v2_x1_0'], [0.01, 0.02, 0.08, 0.03]):\n","    dataloader_train, dataloader_val, dataloader_test, num_classes = set_dataloader(ratio)\n","\n","    save_dir = '/content/drive/MyDrive/KTB/personal_mission/baseline'\n","\n","    evaluate_baseline_model(model_name, save_dir, dataloader_test, device, num_classes)"],"metadata":{"id":"AWl_4IGqt9Xc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**후기 실험**"],"metadata":{"id":"x7pMnug2usg6"}},{"cell_type":"code","source":["# 소수의 dataset으로 구성된 dataloader\n","\n","import torch\n","\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader, random_split\n","\n","def set_dataloader():\n","    # 라벨 목록\n","    labels = os.listdir(root_dir)\n","    label_to_index = {label: idx for idx, label in enumerate(labels)}\n","    num_classes = len(labels)\n","\n","    # 전체 데이터 리스트 생성 (폴더별로 일정 비율 유지)\n","    data_list = {label: [] for label in labels}\n","    for label in labels:\n","        data_dir = os.path.join(root_dir, label)\n","        datas = os.listdir(data_dir)\n","        for img_name in datas:\n","            img_path = os.path.join(data_dir, img_name)\n","            data_list[label].append((img_path, label_to_index[label]))\n","\n","    # 데이터셋 분할 (폴더별로 유지)\n","    train_data, val_data, test_data = [], [], []\n","    for label, items in data_list.items():\n","        random.shuffle(items)\n","        total_size = len(items)\n","\n","        train_data.extend(items[:4])\n","        val_data.extend(items[4:6])\n","        test_data.extend(items[6:])\n","\n","    # 데이터셋 클래스 정의\n","    class CustomImageDataset(Dataset):\n","        def __init__(self, data, transform=None):\n","            self.data = data\n","            self.transform = transform\n","\n","        def __len__(self):\n","            return len(self.data)\n","\n","        def __getitem__(self, idx):\n","            img_path, label = self.data[idx]\n","            img = cv2.imread(img_path)\n","            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","            if self.transform:\n","                img = self.transform(img)\n","\n","            return img, label\n","\n","    # 데이터 변환 정의\n","    transform = transforms.Compose([\n","        transforms.ToPILImage(),\n","        transforms.Resize((224, 224)),  # 이미지 크기 조정\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ])\n","\n","    # 데이터셋 생성\n","    train_dataset = CustomImageDataset(train_data, transform=transform)\n","    val_dataset = CustomImageDataset(val_data, transform=transform)\n","    test_dataset = CustomImageDataset(test_data, transform=transform)\n","\n","    batch_size = 4\n","\n","    # 데이터로더 생성\n","    dataloader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n","    dataloader_val = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n","    dataloader_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n","\n","    # 데이터셋 크기 출력\n","    print(f\"Training Dataset Size: {len(train_dataset)}\")\n","    print(f\"Validation Dataset Size: {len(val_dataset)}\")\n","    print(f\"Test Dataset Size: {len(test_dataset)}\")\n","\n","    # 데이터 확인\n","    for images, labels in dataloader_train:\n","        print(f\"Train Batch Image Shape: {images.shape}\")\n","        print(f\"Train Batch Labels: {labels}\")\n","        break\n","\n","    return dataloader_train, dataloader_val, dataloader_test, num_classes"],"metadata":{"id":"UUKNwpEwuvxy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# augmentation dataloader\n","\n","import torch\n","\n","from PIL import Image\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader, random_split\n","\n","def set_aug_dataloader():\n","    # 라벨 목록\n","    labels = os.listdir(root_dir)\n","    label_to_index = {label: idx for idx, label in enumerate(labels)}\n","    num_classes = len(labels)\n","\n","    # 전체 데이터 리스트 생성 (폴더별로 일정 비율 유지)\n","    data_list = {label: [] for label in labels}\n","    for label in labels:\n","        data_dir = os.path.join(root_dir, label)\n","        datas = os.listdir(data_dir)\n","        for img_name in datas:\n","            img_path = os.path.join(data_dir, img_name)\n","            data_list[label].append((img_path, label_to_index[label]))\n","\n","    # 데이터셋 분할 (폴더별로 유지)\n","    train_data, val_data, test_data = [], [], []\n","    for label, items in data_list.items():\n","        random.shuffle(items)\n","        total_size = len(items)\n","\n","        train_data.extend(items[:4])\n","        val_data.extend(items[4:6])\n","        test_data.extend(items[6:])\n","\n","    # image_list, label_list 분리\n","    train_image_list = []\n","    train_label_list = []\n","\n","    for img_path, label in train_data:\n","        image = Image.open(img_path).convert('RGB')  # 이미지 로딩 및 RGB 변환\n","        train_image_list.append(image)\n","        train_label_list.append(label)\n","\n","    # Augmentation 포함 데이터셋 클래스\n","    class RotatedAugmentedDataset(Dataset):\n","        def __init__(self, image_list, label_list):\n","            self.image_list = image_list\n","            self.label_list = label_list\n","\n","            # 공통 transform (크기 조정 + 정규화)\n","            self.base_transform = transforms.Compose([\n","                transforms.ToTensor(),\n","                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                    std=[0.229, 0.224, 0.225])\n","            ])\n","\n","            # 회전 각도\n","            self.angles = [0, 90, 180, 270]\n","\n","        def __len__(self):\n","            # 데이터 4배로 반환\n","            return len(self.image_list) * 4\n","\n","        def __getitem__(self, idx):\n","            img_idx = idx // 4\n","            angle_idx = idx % 4\n","\n","            image = self.image_list[img_idx]\n","            label = self.label_list[img_idx]\n","\n","            # Resize 먼저\n","            resized_image = image.resize((224, 224))\n","\n","            # 회전 적용\n","            rotated_image = resized_image.rotate(self.angles[angle_idx])\n","\n","            # 정규화 및 텐서 변환\n","            transformed_image = self.base_transform(rotated_image)\n","\n","            return transformed_image, label\n","\n","    # 데이터셋 클래스 정의\n","    class CustomImageDataset(Dataset):\n","        def __init__(self, data, transform=None):\n","            self.data = data\n","            self.transform = transform\n","\n","        def __len__(self):\n","            return len(self.data)\n","\n","        def __getitem__(self, idx):\n","            img_path, label = self.data[idx]\n","            img = cv2.imread(img_path)\n","            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","            if self.transform:\n","                img = self.transform(img)\n","\n","            return img, label\n","\n","    # 데이터 변환 정의\n","    transform = transforms.Compose([\n","        transforms.ToPILImage(),\n","        transforms.Resize((224, 224)),  # 이미지 크기 조정\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ])\n","\n","    # 데이터셋 생성\n","    train_dataset = RotatedAugmentedDataset(train_image_list, train_label_list)\n","    val_dataset = CustomImageDataset(val_data, transform=transform)\n","    test_dataset = CustomImageDataset(test_data, transform=transform)\n","\n","    batch_size = 4\n","\n","    # 데이터로더 생성\n","    dataloader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n","    dataloader_val = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n","    dataloader_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n","\n","    # 데이터셋 크기 출력\n","    print(f\"Training Dataset Size: {len(train_dataset)}\")\n","    print(f\"Validation Dataset Size: {len(val_dataset)}\")\n","    print(f\"Test Dataset Size: {len(test_dataset)}\")\n","\n","    # 데이터 확인\n","    for images, labels in dataloader_train:\n","        print(f\"Train Batch Image Shape: {images.shape}\")\n","        print(f\"Train Batch Labels: {labels}\")\n","        break\n","\n","    return dataloader_train, dataloader_val, dataloader_test, num_classes"],"metadata":{"id":"wPoip19qvkp-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 정의 함수 (Deep CNN 버전)\n","\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.models as models\n","\n","def get_model(model_name, num_classes):\n","    if model_name == 'resnet50':\n","        model = models.resnet50(pretrained=True)\n","        model.fc = nn.Linear(model.fc.in_features, num_classes)\n","    elif model_name == 'resnet152':\n","        model = models.resnet152(pretrained=True)\n","        model.fc = nn.Linear(model.fc.in_features, num_classes)\n","    elif model_name == 'vgg16_bn':\n","        model = models.vgg16_bn(pretrained=True)\n","        model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n","    elif model_name == 'densenet201':\n","        model = models.densenet201(pretrained=True)\n","        model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n","    elif model_name == 'seresnext101':\n","        model = models.resnext101_32x8d(pretrained=True)  # SE-ResNeXt는 torchvision에 없음 → 유사한 구조로 대체\n","        model.fc = nn.Linear(model.fc.in_features, num_classes)\n","    elif model_name == 'convnext_base':\n","        model = models.convnext_base(pretrained=True)\n","        model.classifier[2] = nn.Linear(model.classifier[2].in_features, num_classes)\n","    else:\n","        raise ValueError(f\"Unsupported model: {model_name}\")\n","\n","    return model\n"],"metadata":{"id":"4L64Dz1OvBCv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 평가 함수(학습 함수는 초기 실험 함수 사용)\n","\n","import os\n","import re\n","import torch\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","from torch import nn\n","from torchvision import models\n","from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n","\n","# Root 디렉토리 설정\n","train_root = \"/content/drive/MyDrive/KTB/personal_mission/train_dir\"\n","test_root = \"/content/drive/MyDrive/KTB/personal_mission/test_dir\"\n","\n","# 모든 txt 파일을 찾는 함수\n","def find_txt_files(root_dir):\n","    txt_files = []\n","    for subdir, _, files in os.walk(root_dir):\n","        for file in files:\n","            if file.startswith(\"training_log_\") and file.endswith(\".txt\"):\n","                txt_files.append(os.path.join(subdir, file))\n","    return txt_files\n","\n","# txt 파일에서 학습 로그를 읽고 파싱하는 함수\n","def parse_log(file_path):\n","    epochs, train_loss, val_loss, train_acc, val_acc = [], [], [], [], []\n","\n","    with open(file_path, \"r\") as f:\n","        for line in f:\n","            match = re.search(\n","                r\"Epoch (\\d+)/\\d+, Train Loss: ([\\d.]+), Val Loss: ([\\d.]+), Train Acc: ([\\d.]+), Val Acc: ([\\d.]+)\", line\n","            )\n","            if match:\n","                epoch, t_loss, v_loss, t_acc, v_acc = map(float, match.groups())\n","                epochs.append(int(epoch))\n","                train_loss.append(t_loss)\n","                val_loss.append(v_loss)\n","                train_acc.append(t_acc)\n","                val_acc.append(v_acc)\n","\n","    if not epochs:  # 데이터가 없으면 None 반환\n","        return None\n","\n","    return pd.DataFrame({\n","        \"Epoch\": epochs,\n","        \"Train Loss\": train_loss,\n","        \"Val Loss\": val_loss,\n","        \"Train Acc\": train_acc,\n","        \"Val Acc\": val_acc\n","    })\n","\n","# 파일명에서 모델 이름 추출하는 함수\n","def extract_model_name(file_name):\n","    match = re.search(r\"training_log_(.+)\\.txt\", file_name)\n","    return match.group(1) if match else \"Unknown Model\"\n","\n","# Fine-tuned 모델 불러오는 함수\n","def load_fine_tuned_model(model_name, num_classes, checkpoint_path, device):\n","    try:\n","        if model_name == 'resnet50':\n","            model = models.resnet50(pretrained=True)\n","            model.fc = nn.Linear(model.fc.in_features, num_classes)\n","        elif model_name == 'resnet152':\n","            model = models.resnet152(pretrained=True)\n","            model.fc = nn.Linear(model.fc.in_features, num_classes)\n","        elif model_name == 'vgg16_bn':\n","            model = models.vgg16_bn(pretrained=True)\n","            model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n","        elif model_name == 'densenet201':\n","            model = models.densenet201(pretrained=True)\n","            model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n","        elif model_name == 'seresnext101':\n","            model = models.resnext101_32x8d(pretrained=True)  # SE-ResNeXt는 torchvision에 없음 → 유사한 구조로 대체\n","            model.fc = nn.Linear(model.fc.in_features, num_classes)\n","        elif model_name == 'convnext_base':\n","            model = models.convnext_base(pretrained=True)\n","            model.classifier[2] = nn.Linear(model.classifier[2].in_features, num_classes)\n","        else:\n","            raise ValueError(f\"Unsupported model: {model_name}\")\n","\n","        model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n","        model.to(device)\n","        return model\n","\n","    except Exception as e:\n","        print(f\"⚠️ 모델 {model_name} 로드 중 오류 발생: {e}\")\n","        return None  # 오류 발생 시 None 반환\n","\n","# Confusion Matrix 시각화 및 저장\n","def plot_confusion_matrix(y_true, y_pred, classes, model_name, save_path):\n","    cm = confusion_matrix(y_true, y_pred)\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n","    plt.xlabel('Predicted Label')\n","    plt.ylabel('True Label')\n","    plt.title(f'Confusion Matrix ({model_name})')\n","    plt.savefig(save_path)\n","    plt.close()\n","\n","# 모델 성능 평가 및 Confusion Matrix 저장\n","def evaluate_model(model_name, dir_path, dataloader, device, num_classes):\n","    # Root 디렉토리 설정\n","    train_root = \"/content/drive/MyDrive/KTB/personal_mission/train_dir\"\n","    test_root = \"/content/drive/MyDrive/KTB/personal_mission/test_dir\"\n","\n","    checkpoint_path = os.path.join(dir_path, f\"best_model_{model_name}.pth\")\n","    model = load_fine_tuned_model(model_name, num_classes, checkpoint_path, device)\n","    if model != None:\n","        model.eval()\n","        all_preds, all_labels = [], []\n","\n","        with torch.no_grad():\n","            for images, labels in dataloader:\n","                images, labels = images.to(device), labels.to(device)\n","                outputs = model(images)\n","                _, preds = torch.max(outputs, 1)\n","                all_preds.extend(preds.cpu().numpy())\n","                all_labels.extend(labels.cpu().numpy())\n","\n","        # 데이터셋의 클래스 리스트 가져오기\n","        root_dir = 'almond/dataset'\n","        classes = os.listdir(root_dir)\n","\n","        # 평가 지표 계산\n","        accuracy = accuracy_score(all_labels, all_preds)\n","        precision = precision_score(all_labels, all_preds, average='weighted')\n","        recall = recall_score(all_labels, all_preds, average='weighted')\n","        f1 = f1_score(all_labels, all_preds, average='weighted')\n","\n","        print(f\"Test Accuracy: {accuracy:.4f}\")\n","        print(f\"Test Precision: {precision:.4f}\")\n","        print(f\"Test Recall: {recall:.4f}\")\n","        print(f\"Test F1 Score: {f1:.4f}\")\n","\n","        # 저장 경로 변환 (train_dir -> test_dir)\n","        save_path = dir_path.replace(train_root, test_root)\n","        os.makedirs(save_path, exist_ok=True)\n","        cm_save_path = os.path.join(save_path, f\"confusion_matrix_{model_name}.png\")\n","\n","        plot_confusion_matrix(all_labels, all_preds, classes, model_name, cm_save_path)\n","\n","        # 평가지표 저장\n","        metrics_save_path = os.path.join(save_path, f\"metrics_{model_name}.txt\")\n","        with open(metrics_save_path, \"w\") as f:\n","            f.write(f\"Model: {model_name}\\n\")\n","            f.write(f\"Test Accuracy: {accuracy:.4f}\\n\")\n","            f.write(f\"Test Precision: {precision:.4f}\\n\")\n","            f.write(f\"Test Recall: {recall:.4f}\\n\")\n","            f.write(f\"Test F1 Score: {f1:.4f}\\n\")\n","\n","        print(f\"📄 {metrics_save_path}에 평가지표 저장 완료!\")\n","\n","# 학습 로그 + Confusion Matrix 통합 저장\n","def process_single_experiment(dataloader, device, num_classes, experiment_dir):\n","    \"\"\"\n","    특정 실험 폴더(예: /content/drive/MyDrive/KTB/personal_mission2/train_dir/1)에서만\n","    학습 로그 및 Confusion Matrix를 저장하는 함수\n","    \"\"\"\n","\n","    # train_dir에서 test_dir로 변환\n","    test_experiment_dir = experiment_dir.replace(\"train_dir\", \"test_dir\")\n","\n","    # experiment_dir 존재 여부 확인\n","    if not os.path.exists(experiment_dir):\n","        print(f\"⚠️ 유효하지 않은 실험 디렉토리: {experiment_dir}\")\n","        return\n","\n","    # 해당 실험 폴더 안의 training_log 파일 찾기\n","    txt_files = [os.path.join(experiment_dir, file) for file in os.listdir(experiment_dir) if file.startswith(\"training_log_\") and file.endswith(\".txt\")]\n","\n","    if not txt_files:\n","        print(f\"❌ {experiment_dir}에서 training_log 파일을 찾을 수 없습니다.\")\n","        return\n","\n","    os.makedirs(test_experiment_dir, exist_ok=True)  # test_dir에 해당 실험 폴더 생성\n","\n","    for txt_file in txt_files:\n","        df = parse_log(txt_file)\n","        if df is not None:\n","            model_name = extract_model_name(os.path.basename(txt_file))\n","\n","            # 학습 로그 그래프 저장\n","            plt.figure(figsize=(12, 5))\n","\n","            # Loss 그래프\n","            plt.subplot(1, 2, 1)\n","            plt.plot(df[\"Epoch\"], df[\"Train Loss\"], label=\"Train Loss\", marker=\"o\")\n","            plt.plot(df[\"Epoch\"], df[\"Val Loss\"], label=\"Val Loss\", marker=\"s\")\n","            plt.xlabel(\"Epoch\")\n","            plt.ylabel(\"Loss\")\n","            plt.title(f\"Training & Validation Loss ({model_name})\")\n","            plt.legend()\n","            plt.grid(True)\n","\n","            # Accuracy 그래프\n","            plt.subplot(1, 2, 2)\n","            plt.plot(df[\"Epoch\"], df[\"Train Acc\"], label=\"Train Acc\", marker=\"o\")\n","            plt.plot(df[\"Epoch\"], df[\"Val Acc\"], label=\"Val Acc\", marker=\"s\")\n","            plt.xlabel(\"Epoch\")\n","            plt.ylabel(\"Accuracy\")\n","            plt.title(f\"Training & Validation Accuracy ({model_name})\")\n","            plt.legend()\n","            plt.grid(True)\n","\n","            plt.tight_layout()\n","            plt.savefig(os.path.join(test_experiment_dir, f\"training_log_{model_name}.png\"))\n","            plt.close()\n","\n","            # Confusion Matrix 및 성능 지표 저장\n","            evaluate_model(model_name, experiment_dir, dataloader, device, num_classes)\n","\n","    print(f\"✅ {experiment_dir}의 학습 로그 및 Confusion Matrix가 {test_experiment_dir}에 저장되었습니다.\")\n","\n"],"metadata":{"id":"WeD7Z-YEw9mc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Cross Validation K-Fold(DB 연동)**"],"metadata":{"id":"CkQ0iF7a1Q4V"}},{"cell_type":"code","source":["# CV K-Fold DB Table, DB Update 함수\n","\n","import sqlite3\n","from datetime import datetime\n","\n","def create_experiment_db(db_path=\"/content/drive/MyDrive/KTB/personal_mission/experiment_log.db\"):\n","    conn = sqlite3.connect(db_path)\n","    cursor = conn.cursor()\n","\n","    # 1. experiments: 전체 실험 정보\n","    cursor.execute(\"\"\"\n","    CREATE TABLE IF NOT EXISTS experiments (\n","        id INTEGER PRIMARY KEY AUTOINCREMENT,\n","        model_name TEXT,\n","        total_epochs INTEGER,\n","        k_fold INTEGER,\n","        lr REAL,\n","        patience INTEGER,\n","        created_at TEXT\n","    )\n","    \"\"\")\n","\n","    # 2. experiment_folds: 각 fold별 정보\n","    cursor.execute(\"\"\"\n","    CREATE TABLE IF NOT EXISTS experiment_folds (\n","        id INTEGER PRIMARY KEY AUTOINCREMENT,\n","        experiment_id INTEGER,\n","        fold INTEGER,\n","        train_size INTEGER,\n","        val_size INTEGER,\n","        best_model_path TEXT,\n","        early_stopped_epoch INTEGER,\n","        log_plot_path TEXT,\n","        FOREIGN KEY (experiment_id) REFERENCES experiments (id)\n","    )\n","    \"\"\")\n","\n","    # 3. experiment_logs: 학습 로그 (에폭별)\n","    cursor.execute(\"\"\"\n","    CREATE TABLE IF NOT EXISTS experiment_logs (\n","        id INTEGER PRIMARY KEY AUTOINCREMENT,\n","        experiment_id INTEGER,\n","        fold INTEGER,\n","        epoch INTEGER,\n","        train_loss REAL,\n","        val_loss REAL,\n","        train_acc REAL,\n","        val_acc REAL,\n","        FOREIGN KEY (experiment_id) REFERENCES experiments (id)\n","    )\n","    \"\"\")\n","\n","    # 4. experiment_tests: 테스트 성능 기록\n","    cursor.execute(\"\"\"\n","    CREATE TABLE IF NOT EXISTS experiment_tests (\n","        id INTEGER PRIMARY KEY AUTOINCREMENT,\n","        experiment_id INTEGER,\n","        fold INTEGER,\n","        is_best_fold INTEGER DEFAULT 0,\n","        is_ensemble INTEGER DEFAULT 0,\n","        test_acc REAL,\n","        test_precision REAL,\n","        test_recall REAL,\n","        test_f1 REAL,\n","        conf_matrix_path TEXT,\n","        FOREIGN KEY (experiment_id) REFERENCES experiments (id)\n","    )\n","    \"\"\")\n","\n","    conn.commit()\n","    conn.close()\n","    print(\"✅ SQLite DB 및 테이블 생성 완료\")\n","\n","\n","def insert_experiment_with_folds(db_path, model_name, total_epochs, k_fold, lr, patience, fold_data_sizes):\n","    \"\"\"\n","    fold_data_sizes: List of (train_size, val_size) for each fold\n","    \"\"\"\n","    conn = sqlite3.connect(db_path)\n","    cursor = conn.cursor()\n","\n","    created_at = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","\n","    # 1. experiments 테이블에 등록\n","    cursor.execute(\"\"\"\n","        INSERT INTO experiments (\n","            model_name, total_epochs, k_fold, lr, patience, created_at\n","        ) VALUES (?, ?, ?, ?, ?, ?)\n","    \"\"\", (model_name, total_epochs, k_fold, lr, patience, created_at))\n","\n","    experiment_id = cursor.lastrowid\n","\n","    # 2. experiment_folds 테이블에 k개 fold 등록\n","    fold_id_list = []\n","\n","    for i, (train_size, val_size) in enumerate(fold_data_sizes):\n","        cursor.execute(\"\"\"\n","            INSERT INTO experiment_folds (\n","                experiment_id, fold, train_size, val_size\n","            ) VALUES (?, ?, ?, ?)\n","        \"\"\", (experiment_id, i+1, train_size, val_size))\n","        fold_id_list.append(cursor.lastrowid)\n","\n","    conn.commit()\n","    conn.close()\n","\n","    print(f\"✅ 실험 등록 완료 (experiment_id: {experiment_id})\")\n","    return experiment_id, fold_id_list\n","\n","\n","def insert_epoch_log(db_path, experiment_id, fold, epoch, train_loss, val_loss, train_acc, val_acc):\n","    conn = sqlite3.connect(db_path)\n","    cursor = conn.cursor()\n","\n","    cursor.execute(\"\"\"\n","        INSERT INTO experiment_logs (\n","            experiment_id, fold, epoch, train_loss, val_loss, train_acc, val_acc\n","        ) VALUES (?, ?, ?, ?, ?, ?, ?)\n","    \"\"\", (experiment_id, fold, epoch, train_loss, val_loss, train_acc, val_acc))\n","\n","    conn.commit()\n","    conn.close()\n","\n","\n","def update_fold_info(db_path, experiment_id, fold, best_model_path=None, early_stopped_epoch=None, log_plot_path=None):\n","    conn = sqlite3.connect(db_path)\n","    cursor = conn.cursor()\n","\n","    if best_model_path:\n","        cursor.execute(\"\"\"\n","            UPDATE experiment_folds\n","            SET best_model_path = ?\n","            WHERE experiment_id = ? AND fold = ?\n","        \"\"\", (best_model_path, experiment_id, fold))\n","\n","    if early_stopped_epoch:\n","        cursor.execute(\"\"\"\n","            UPDATE experiment_folds\n","            SET early_stopped_epoch = ?\n","            WHERE experiment_id = ? AND fold = ?\n","        \"\"\", (early_stopped_epoch, experiment_id, fold))\n","\n","    if log_plot_path:\n","        cursor.execute(\"\"\"\n","            UPDATE experiment_folds\n","            SET log_plot_path = ?\n","            WHERE experiment_id = ? AND fold = ?\n","        \"\"\", (log_plot_path, experiment_id, fold))\n","\n","    conn.commit()\n","    conn.close()\n","\n","\n","def insert_test_result(\n","    db_path,\n","    experiment_id,\n","    fold,\n","    is_best_fold,\n","    is_ensemble,\n","    test_acc,\n","    test_precision,\n","    test_recall,\n","    test_f1,\n","    conf_matrix_path\n","):\n","    conn = sqlite3.connect(db_path)\n","    cursor = conn.cursor()\n","\n","    cursor.execute(\"\"\"\n","        INSERT INTO experiment_tests (\n","            experiment_id, fold, is_best_fold, is_ensemble,\n","            test_acc, test_precision, test_recall, test_f1, conf_matrix_path\n","        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n","    \"\"\", (\n","        experiment_id, fold, is_best_fold, is_ensemble,\n","        test_acc, test_precision, test_recall, test_f1, conf_matrix_path\n","    ))\n","\n","    conn.commit()\n","    conn.close()\n","\n","\n","def plot_log_from_db(db_path, experiment_id, fold, model_name, save_path):\n","    conn = sqlite3.connect(db_path)\n","\n","    # 로그 조회\n","    df = pd.read_sql_query(\"\"\"\n","        SELECT epoch, train_loss, val_loss, train_acc, val_acc\n","        FROM experiment_logs\n","        WHERE experiment_id = ? AND fold = ?\n","        ORDER BY epoch\n","    \"\"\", conn, params=(experiment_id, fold))\n","\n","    conn.close()\n","\n","    if df.empty:\n","        print(f\"❌ 로그 데이터가 존재하지 않습니다. (experiment_id={experiment_id}, fold={fold})\")\n","        return\n","\n","    # 그래프 그리기\n","    plt.figure(figsize=(12, 5))\n","\n","    # Loss 그래프\n","    plt.subplot(1, 2, 1)\n","    plt.plot(df['epoch'], df['train_loss'], label='Train Loss', marker='o')\n","    plt.plot(df['epoch'], df['val_loss'], label='Val Loss', marker='s')\n","    plt.title(f\"Loss Curve - {model_name} (Fold {fold})\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend()\n","    plt.grid(True)\n","\n","    # Accuracy 그래프\n","    plt.subplot(1, 2, 2)\n","    plt.plot(df['epoch'], df['train_acc'], label='Train Acc', marker='o')\n","    plt.plot(df['epoch'], df['val_acc'], label='Val Acc', marker='s')\n","    plt.title(f\"Accuracy Curve - {model_name} (Fold {fold})\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Accuracy\")\n","    plt.legend()\n","    plt.grid(True)\n","\n","    plt.tight_layout()\n","    plt.savefig(save_path)\n","    plt.close()\n","    print(f\"📈 Fold {fold} 학습 그래프 저장 완료: {save_path}\")\n","\n","\n","def get_best_fold_path(db_path, experiment_id):\n","    import sqlite3\n","    conn = sqlite3.connect(db_path)\n","    cursor = conn.cursor()\n","\n","    # 1. 각 fold별 평균 val_loss 계산\n","    cursor.execute(\"\"\"\n","        SELECT fold, AVG(val_loss) as avg_val_loss\n","        FROM experiment_logs\n","        WHERE experiment_id = ?\n","        GROUP BY fold\n","        ORDER BY avg_val_loss ASC\n","        LIMIT 1\n","    \"\"\", (experiment_id,))\n","    result = cursor.fetchone()\n","\n","    if not result:\n","        return None, None\n","\n","    best_fold = result[0]\n","\n","    # 2. best_model_path 조회\n","    cursor.execute(\"\"\"\n","        SELECT best_model_path FROM experiment_folds\n","        WHERE experiment_id = ? AND fold = ?\n","    \"\"\", (experiment_id, best_fold))\n","    model_path_result = cursor.fetchone()\n","\n","    conn.close()\n","\n","    if model_path_result:\n","        return best_fold, model_path_result[0]\n","    else:\n","        return best_fold, None\n"],"metadata":{"id":"cpYmEuMD1UBA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cross Validation K-Fold 전용 dataloader\n","\n","import torch\n","\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader, random_split\n","\n","def set_dataloader():\n","    # 라벨 목록\n","    labels = os.listdir(root_dir)\n","    label_to_index = {label: idx for idx, label in enumerate(labels)}\n","    num_classes = len(labels)\n","\n","    # 전체 데이터 리스트 생성 (폴더별로 일정 비율 유지)\n","    data_list = {label: [] for label in labels}\n","    for label in labels:\n","        data_dir = os.path.join(root_dir, label)\n","        datas = os.listdir(data_dir)\n","        for img_name in datas:\n","            img_path = os.path.join(data_dir, img_name)\n","            data_list[label].append((img_path, label_to_index[label]))\n","\n","    # 데이터셋 분할 (폴더별로 유지)\n","    train_data, test_data = [], []\n","    for label, items in data_list.items():\n","        random.shuffle(items)\n","        total_size = len(items)\n","\n","        train_data.extend(items[:6])\n","        test_data.extend(items[6:])\n","\n","    # 데이터셋 클래스 정의\n","    class CustomImageDataset(Dataset):\n","        def __init__(self, data, transform=None):\n","            self.data = data\n","            self.transform = transform\n","\n","        def __len__(self):\n","            return len(self.data)\n","\n","        def __getitem__(self, idx):\n","            img_path, label = self.data[idx]\n","            img = cv2.imread(img_path)\n","            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","            if self.transform:\n","                img = self.transform(img)\n","\n","            return img, label\n","\n","    # 데이터 변환 정의\n","    transform = transforms.Compose([\n","        transforms.ToPILImage(),\n","        transforms.Resize((224, 224)),  # 이미지 크기 조정\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ])\n","\n","    # 데이터셋 생성\n","    train_dataset = CustomImageDataset(train_data, transform=transform)\n","    test_dataset = CustomImageDataset(test_data, transform=transform)\n","\n","    batch_size = 4\n","\n","    # 데이터로더 생성\n","    dataloader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n","    dataloader_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n","\n","    # 데이터셋 크기 출력\n","    print(f\"Training Dataset Size: {len(train_dataset)}\")\n","    print(f\"Test Dataset Size: {len(test_dataset)}\")\n","\n","    # 데이터 확인\n","    for images, labels in dataloader_train:\n","        print(f\"Train Batch Image Shape: {images.shape}\")\n","        print(f\"Train Batch Labels: {labels}\")\n","        break\n","\n","    return train_dataset, dataloader_train, dataloader_test, num_classes"],"metadata":{"id":"KrR6Gu4DzOvO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cross Validation K-Fold 학습 코드\n","\n","from sklearn.model_selection import StratifiedKFold\n","from tqdm import tqdm\n","import torch.optim as optim\n","import os\n","import torch.nn as nn\n","import torch\n","\n","def train_model_cv_kfold(\n","    model_name, dataset, num_classes, save_dir, device,\n","    k=5, epochs=10, lr=0.0001, patience=3,\n","    db_path=\"/content/drive/MyDrive/KTB/personal_mission/experiment_log.db\"\n","):\n","    # Stratified K-Fold 준비\n","    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n","    targets = [label for _, label in dataset]  # label만 추출\n","\n","    # fold별 데이터 사이즈 파악\n","    fold_data_sizes = []\n","    fold_indices = []\n","    for train_idx, val_idx in skf.split(dataset, targets):\n","        fold_data_sizes.append((len(train_idx), len(val_idx)))\n","        fold_indices.append((train_idx, val_idx))\n","\n","    # ✅ 실험 등록 및 fold ID 초기화\n","    experiment_id, _ = insert_experiment_with_folds(\n","        db_path=db_path,\n","        model_name=model_name,\n","        total_epochs=epochs,\n","        k_fold=k,\n","        lr=lr,\n","        patience=patience,\n","        fold_data_sizes=fold_data_sizes\n","    )\n","\n","    # fold별 모델, 옵티마이저, 데이터로더 초기화\n","    fold_models, fold_optimizers = [], []\n","    fold_train_loaders, fold_val_loaders = [], []\n","\n","    for i, (train_idx, val_idx) in enumerate(fold_indices):\n","        train_subset = torch.utils.data.Subset(dataset, train_idx)\n","        val_subset = torch.utils.data.Subset(dataset, val_idx)\n","\n","        print(f\"📊 Fold {i+1} - Train Size: {len(train_subset)}, Validation Size: {len(val_idx)}\")\n","\n","        model = get_model(model_name, num_classes).to(device)\n","        optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","        train_loader = torch.utils.data.DataLoader(train_subset, batch_size=4, shuffle=True)\n","        val_loader = torch.utils.data.DataLoader(val_subset, batch_size=4, shuffle=False)\n","\n","        fold_models.append(model)\n","        fold_optimizers.append(optimizer)\n","        fold_train_loaders.append(train_loader)\n","        fold_val_loaders.append(val_loader)\n","\n","    # 학습 루프\n","    best_avg_val_loss = float(\"inf\")\n","    no_improve_count = 0\n","\n","    for epoch in tqdm(range(epochs), desc=f\"[{model_name} CV-KFold]\"):\n","        fold_val_losses, fold_val_accs = [], []\n","\n","        for fold in range(k):\n","            model = fold_models[fold]\n","            optimizer = fold_optimizers[fold]\n","            train_loader = fold_train_loaders[fold]\n","            val_loader = fold_val_loaders[fold]\n","\n","            # 학습\n","            train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, device)\n","\n","            # 검증\n","            val_loss, val_acc = validate_model(model, val_loader, device, nn.CrossEntropyLoss())\n","\n","            # ✅ 로그 저장\n","            insert_epoch_log(db_path, experiment_id, fold+1, epoch+1, train_loss, val_loss, train_acc, val_acc)\n","\n","            fold_val_losses.append(val_loss)\n","            fold_val_accs.append(val_acc)\n","\n","        # Cross-Fold 평균으로 early stop 감시\n","        avg_val_loss = sum(fold_val_losses) / k\n","\n","        if avg_val_loss < best_avg_val_loss:\n","            best_avg_val_loss = avg_val_loss\n","            no_improve_count = 0\n","\n","            # ✅ fold별 모델 저장\n","            for fold in range(k):\n","                model_path = os.path.join(save_dir, f\"best_model_{model_name}_fold{fold+1}.pth\")\n","                torch.save(fold_models[fold].state_dict(), model_path)\n","\n","        else:\n","            no_improve_count += 1\n","\n","        if no_improve_count >= patience:\n","            print(f\"🛑 Early stopping at epoch {epoch+1}\")\n","            break\n","\n","    # ✅ fold별 학습 종료 후 정보 업데이트\n","    for fold in range(k):\n","        model_path = os.path.join(save_dir, f\"best_model_{model_name}_fold{fold+1}.pth\")\n","        log_plot_path = os.path.join(save_dir, f\"training_plot_{model_name}_fold{fold+1}.png\")\n","\n","        # 그래프 저장\n","        plot_log_from_db(\n","            db_path=db_path,\n","            experiment_id=experiment_id,\n","            fold=fold + 1,  # ✅ 숫자형 fold index\n","            model_name=model_name,\n","            save_path=log_plot_path\n","        )\n","\n","        # fold 정보 업데이트\n","        update_fold_info(\n","            db_path,\n","            experiment_id,\n","            fold=fold+1,\n","            best_model_path=model_path,\n","            early_stopped_epoch=epoch+1,\n","            log_plot_path=log_plot_path\n","        )\n","\n","    print(f\"\\n✅ {model_name} 모델의 CV K-Fold 학습 완료 (experiment_id: {experiment_id})\")\n","    return experiment_id\n","\n","def train_one_epoch(model, dataloader, optimizer, device):\n","    model.train()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    criterion = nn.CrossEntropyLoss()\n","\n","    for images, labels in dataloader:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        _, predicted = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    avg_loss = running_loss / len(dataloader)\n","    accuracy = correct / total\n","    return avg_loss, accuracy\n","\n","def validate_model(model, dataloader, device, criterion):\n","    model.eval()\n","    total_loss = 0.0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for images, labels in dataloader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item()\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    avg_loss = total_loss / len(dataloader)\n","    accuracy = correct / total\n","    return avg_loss, accuracy"],"metadata":{"id":"pMI7Vbbe0CIx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 평가 함수\n","\n","import seaborn as sns\n","import torch.nn as nn\n","import torchvision.models as models\n","\n","from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n","\n","# Fine-tuned 모델 불러오는 함수\n","def load_fine_tuned_model(model_name, num_classes, checkpoint_path, device):\n","    try:\n","        if model_name == 'resnet50':\n","            model = models.resnet50(pretrained=True)\n","            model.fc = nn.Linear(model.fc.in_features, num_classes)\n","        elif model_name == 'resnet152':\n","            model = models.resnet152(pretrained=True)\n","            model.fc = nn.Linear(model.fc.in_features, num_classes)\n","        elif model_name == 'vgg16_bn':\n","            model = models.vgg16_bn(pretrained=True)\n","            model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n","        elif model_name == 'densenet201':\n","            model = models.densenet201(pretrained=True)\n","            model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n","        elif model_name == 'seresnext101':\n","            model = models.resnext101_32x8d(pretrained=True)  # SE-ResNeXt는 torchvision에 없음 → 유사한 구조로 대체\n","            model.fc = nn.Linear(model.fc.in_features, num_classes)\n","        elif model_name == 'convnext_base':\n","            model = models.convnext_base(pretrained=True)\n","            model.classifier[2] = nn.Linear(model.classifier[2].in_features, num_classes)\n","\n","        model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n","        model.to(device)\n","        return model\n","\n","    except Exception as e:\n","        print(f\"⚠️ 모델 {model_name} 로드 중 오류 발생: {e}\")\n","        return None  # 오류 발생 시 None 반환\n","\n","# Confusion Matrix 시각화 및 저장\n","def plot_confusion_matrix(y_true, y_pred, classes, model_name, save_path):\n","    cm = confusion_matrix(y_true, y_pred)\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n","    plt.xlabel('Predicted Label')\n","    plt.ylabel('True Label')\n","    plt.title(f'Confusion Matrix ({model_name})')\n","    plt.savefig(save_path)\n","    plt.close()\n","\n","def evaluate_model_and_log(\n","    experiment_id,\n","    model_name,\n","    test_loader,\n","    num_classes,\n","    device,\n","    save_dir,\n","    fold=None,\n","    is_best_fold=0,\n","    is_ensemble=0,\n","    db_path=\"/content/drive/MyDrive/KTB/personal_mission/experiment_log.db\",\n","    class_dir='almond/dataset'\n","):\n","    try:\n","        best_fold, model_path = get_best_fold_path(db_path, experiment_id)\n","        fold = best_fold\n","\n","        # 모델 불러오기\n","        model = load_fine_tuned_model(model_name, num_classes, model_path, device)\n","        if model is None:\n","            print(f\"❌ 모델 {model_path} 로드 실패\")\n","            return\n","\n","        model.eval()\n","        all_preds, all_labels = [], []\n","\n","        with torch.no_grad():\n","            for images, labels in test_loader:\n","                images, labels = images.to(device), labels.to(device)\n","                outputs = model(images)\n","                _, preds = torch.max(outputs, 1)\n","                all_preds.extend(preds.cpu().numpy())\n","                all_labels.extend(labels.cpu().numpy())\n","\n","        # 클래스명 (Confusion Matrix용)\n","        class_names = os.listdir(class_dir)\n","\n","        # 지표 계산\n","        acc = accuracy_score(all_labels, all_preds)\n","        precision = precision_score(all_labels, all_preds, average='weighted')\n","        recall = recall_score(all_labels, all_preds, average='weighted')\n","        f1 = f1_score(all_labels, all_preds, average='weighted')\n","\n","        print(f\"✅ 평가 완료 - Acc: {acc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n","\n","        # Confusion Matrix 시각화\n","        cm = confusion_matrix(all_labels, all_preds)\n","        plt.figure(figsize=(8, 6))\n","        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n","        plt.xlabel('Predicted Label')\n","        plt.ylabel('True Label')\n","        plt.title(f'Confusion Matrix ({model_name})')\n","\n","        # 저장 경로 생성 및 저장\n","        os.makedirs(save_dir, exist_ok=True)\n","        suffix = \"ensemble\" if is_ensemble else (f\"best_fold{fold}\" if is_best_fold else f\"fold{fold}\")\n","        cm_path = os.path.join(save_dir, f\"conf_matrix_{model_name}_{suffix}.png\")\n","        plt.savefig(cm_path)\n","        plt.close()\n","\n","        # ✅ DB 저장\n","        insert_test_result(\n","            db_path=db_path,\n","            experiment_id=experiment_id,\n","            fold=fold if fold is not None else 0,\n","            is_best_fold=is_best_fold,\n","            is_ensemble=is_ensemble,\n","            test_acc=acc,\n","            test_precision=precision,\n","            test_recall=recall,\n","            test_f1=f1,\n","            conf_matrix_path=cm_path\n","        )\n","\n","    except Exception as e:\n","        print(f\"❌ 평가 중 오류 발생: {e}\")\n","\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# 각 fold 평가\n","def evaluate_fold_models(\n","    experiment_id,\n","    model_name,\n","    k,\n","    save_dir,\n","    test_loader,\n","    num_classes,\n","    device,\n","    db_path=\"/content/drive/MyDrive/KTB/personal_mission/experiment_log.db\"\n","):\n","    best_acc = 0\n","    best_fold = None\n","\n","    for fold in range(1, k+1):  # fold 1~5\n","        model_path = os.path.join(save_dir, f\"best_model_{model_name}_fold{fold}.pth\")\n","        model = load_fine_tuned_model(model_name, num_classes, model_path, device)\n","        if model is None:\n","            continue\n","\n","        model.eval()\n","        all_preds, all_labels = [], []\n","\n","        with torch.no_grad():\n","            for images, labels in test_loader:\n","                images, labels = images.to(device), labels.to(device)\n","                outputs = model(images)\n","                _, preds = torch.max(outputs, 1)\n","                all_preds.extend(preds.cpu().numpy())\n","                all_labels.extend(labels.cpu().numpy())\n","\n","        acc = accuracy_score(all_labels, all_preds)\n","        precision = precision_score(all_labels, all_preds, average='weighted')\n","        recall = recall_score(all_labels, all_preds, average='weighted')\n","        f1 = f1_score(all_labels, all_preds, average='weighted')\n","\n","        if acc > best_acc:\n","            best_acc = acc\n","            best_fold = fold\n","\n","        # 클래스명 (Confusion Matrix용)\n","        class_dir='almond/dataset'\n","        class_names = os.listdir(class_dir)\n","\n","        # Confusion matrix 저장\n","        cm = confusion_matrix(all_labels, all_preds)\n","        plt.figure(figsize=(8, 6))\n","        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n","        plt.title(f'Confusion Matrix - Fold {fold}')\n","        plt.xlabel('Predicted Label')\n","        plt.ylabel('True Label')\n","\n","        cm_path = os.path.join(save_dir, f\"conf_matrix_{model_name}_fold{fold}.png\")\n","        plt.savefig(cm_path)\n","        plt.close()\n","\n","        # ✅ DB 저장\n","        insert_test_result(\n","            db_path=db_path,\n","            experiment_id=experiment_id,\n","            fold=fold,\n","            is_best_fold=0,\n","            is_ensemble=0,\n","            test_acc=acc,\n","            test_precision=precision,\n","            test_recall=recall,\n","            test_f1=f1,\n","            conf_matrix_path=cm_path\n","        )\n","\n","    return best_fold\n","\n","# Val loss 기준 best fold 모델 평가\n","def evaluate_best_fold_model(\n","    experiment_id,\n","    model_name,\n","    save_dir,\n","    test_loader,\n","    num_classes,\n","    device,\n","    db_path=\"/content/drive/MyDrive/KTB/personal_mission/experiment_log.db\"\n","):\n","    best_fold, model_path = get_best_fold_path(db_path, experiment_id)\n","    model = load_fine_tuned_model(model_name, num_classes, model_path, device)\n","    if model is None:\n","        return\n","\n","    model.eval()\n","    all_preds, all_labels = [], []\n","\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, preds = torch.max(outputs, 1)\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    acc = accuracy_score(all_labels, all_preds)\n","    precision = precision_score(all_labels, all_preds, average='weighted')\n","    recall = recall_score(all_labels, all_preds, average='weighted')\n","    f1 = f1_score(all_labels, all_preds, average='weighted')\n","\n","    # 클래스명 (Confusion Matrix용)\n","    class_dir='almond/dataset'\n","    class_names = os.listdir(class_dir)\n","\n","    # Confusion matrix 저장\n","    cm = confusion_matrix(all_labels, all_preds)\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n","    plt.title(f'Best Fold Confusion Matrix')\n","    plt.xlabel('Predicted Label')\n","    plt.ylabel('True Label')\n","\n","\n","    cm_path = os.path.join(save_dir, f\"conf_matrix_{model_name}_best_fold{best_fold}.png\")\n","    plt.savefig(cm_path)\n","    plt.close()\n","\n","    # ✅ DB 저장\n","    insert_test_result(\n","        db_path=db_path,\n","        experiment_id=experiment_id,\n","        fold=best_fold,\n","        is_best_fold=1,\n","        is_ensemble=0,\n","        test_acc=acc,\n","        test_precision=precision,\n","        test_recall=recall,\n","        test_f1=f1,\n","        conf_matrix_path=cm_path\n","    )\n","\n","import numpy as np\n","\n","# 앙상블 모델 평가\n","def evaluate_ensemble_model(\n","    experiment_id,\n","    model_name,\n","    k,\n","    save_dir,\n","    test_loader,\n","    num_classes,\n","    device,\n","    db_path=\"/content/drive/MyDrive/KTB/personal_mission/experiment_log.db\"\n","):\n","    models_list = []\n","    for fold in range(1, k+1):\n","        model_path = os.path.join(save_dir, f\"best_model_{model_name}_fold{fold}.pth\")\n","        model = load_fine_tuned_model(model_name, num_classes, model_path, device)\n","        if model:\n","            model.eval()\n","            models_list.append(model)\n","\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images = images.to(device)\n","            labels = labels.cpu().numpy()\n","            outputs_sum = None\n","\n","            for model in models_list:\n","                outputs = model(images)\n","                probs = torch.softmax(outputs, dim=1).cpu().numpy()\n","                outputs_sum = probs if outputs_sum is None else outputs_sum + probs\n","\n","            avg_probs = outputs_sum / len(models_list)\n","            preds = np.argmax(avg_probs, axis=1)\n","\n","            all_preds.extend(preds)\n","            all_labels.extend(labels)\n","\n","    acc = accuracy_score(all_labels, all_preds)\n","    precision = precision_score(all_labels, all_preds, average='weighted')\n","    recall = recall_score(all_labels, all_preds, average='weighted')\n","    f1 = f1_score(all_labels, all_preds, average='weighted')\n","\n","    # 클래스명 (Confusion Matrix용)\n","    class_dir='almond/dataset'\n","    class_names = os.listdir(class_dir)\n","\n","    # Confusion matrix 저장\n","    cm = confusion_matrix(all_labels, all_preds)\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n","    plt.title(f'Ensemble Confusion Matrix')\n","    plt.xlabel('Predicted Label')\n","    plt.ylabel('True Label')\n","\n","    cm_path = os.path.join(save_dir, f\"conf_matrix_{model_name}_ensemble.png\")\n","    plt.savefig(cm_path)\n","    plt.close()\n","\n","    # ✅ DB 저장\n","    insert_test_result(\n","        db_path=db_path,\n","        experiment_id=experiment_id,\n","        fold=0,  # 앙상블은 fold 없음\n","        is_best_fold=0,\n","        is_ensemble=1,\n","        test_acc=acc,\n","        test_precision=precision,\n","        test_recall=recall,\n","        test_f1=f1,\n","        conf_matrix_path=cm_path\n","    )\n","\n"],"metadata":{"id":"gKC286ZS0TqS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fine-tuning\n","dataloader_train, dataloader_val, dataloader_test, num_classes = set_dataloader()\n","\n","save_dir = create_next_directory('/content/drive/MyDrive/KTB/personal_mission/train_dir')\n","for model_name in ['resnet50', 'resnet152', 'vgg16_bn', 'densenet201', 'seresnext101', 'convnext_base']:\n","    print(f\"Training {model_name}...\")\n","    train_model(model_name, dataloader_train, dataloader_val, num_classes, save_dir, device, epochs=200, lr=0.00005, patience=10)\n","process_single_experiment(dataloader_test, device, num_classes, save_dir)\n","\n","\n","# Augmentation + Fine-tuning\n","dataloader_train, dataloader_val, dataloader_test, num_classes = set_aug_dataloader()\n","\n","save_dir = create_next_directory('/content/drive/MyDrive/KTB/personal_mission/aug_train_dir')\n","for model_name in ['resnet50', 'resnet152', 'vgg16_bn', 'densenet201', 'seresnext101', 'convnext_base']:\n","    print(f\"Training {model_name}...\")\n","    train_model(model_name, dataloader_train, dataloader_val, num_classes, save_dir, device, epochs=200, lr=0.00005, patience=10)\n","process_single_experiment(dataloader_test, device, num_classes, save_dir)\n","\n","# C-V K-Fold + Fine-tuning\n","train_dataset, dataloader_train, dataloader_test, num_classes = set_dataloader()\n","\n","save_dir = create_next_directory('/content/drive/MyDrive/KTB/personal_mission/k-fold_aug_train_dir')\n","for model_name in ['resnet50', 'resnet152', 'vgg16_bn', 'densenet201', 'seresnext101', 'convnext_base']:\n","    print(f\"Training {model_name}...\")\n","    experiment_id =  train_model_cv_kfold(model_name, train_dataset, num_classes, save_dir, device, k=3, epochs=200, lr=0.00005, patience=10)\n","    print(f\"Testing {model_name}...\")\n","    evaluate_model_and_log(experiment_id, model_name, dataloader_test, num_classes, device, save_dir)\n","    evaluate_fold_models(experiment_id, model_name, 3, save_dir, dataloader_test, num_classes, device, db_path=\"/content/drive/MyDrive/KTB/personal_mission/experiment_log.db\")\n","    evaluate_best_fold_model(experiment_id, model_name, save_dir, dataloader_test, num_classes, device, db_path=\"/content/drive/MyDrive/KTB/personal_mission/experiment_log.db\")\n","    evaluate_ensemble_model(experiment_id, model_name, 3, save_dir, dataloader_test, num_classes, device, db_path=\"/content/drive/MyDrive/KTB/personal_mission/experiment_log.db\")"],"metadata":{"id":"Ly5fTtFE0V-S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Data Augmentation + Cross Validation K-Fold**"],"metadata":{"id":"ctaPZeF82HRD"}},{"cell_type":"code","source":["# Augmentation 포함 Dataset 클래스\n","\n","from PIL import Image\n","from torchvision.transforms.functional import rotate\n","\n","class RotatedAugmentedDataset(Dataset):\n","    def __init__(self, original_dataset, indices):\n","        self.original_dataset = original_dataset\n","        self.indices = indices\n","        self.angles = [0, 90, 180, 270]\n","\n","    def __len__(self):\n","        return len(self.indices) * 4\n","\n","    def __getitem__(self, idx):\n","        img_idx = idx // 4\n","        angle_idx = idx % 4\n","\n","        image = self.original_dataset[img_idx][0]\n","        label = self.original_dataset[img_idx][1]\n","\n","        rotated_image = rotate(image, self.angles[idx % 4])\n","\n","        return rotated_image, label"],"metadata":{"id":"cPJGlU7N2Ml2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Augmentation + Cross Validation K-Fold 학습 코드\n","\n","from sklearn.model_selection import StratifiedKFold\n","from tqdm import tqdm\n","import torch.optim as optim\n","import os\n","import torch.nn as nn\n","import torch\n","\n","def train_model_cv_kfold(\n","    model_name, dataset, num_classes, save_dir, device,\n","    k=5, epochs=10, lr=0.0001, patience=3,\n","    db_path=\"/content/drive/MyDrive/KTB/personal_mission/experiment_log.db\"\n","):\n","    # Stratified K-Fold 준비\n","    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n","    targets = [label for _, label in dataset]  # label만 추출\n","\n","    # fold별 데이터 사이즈 파악\n","    fold_data_sizes = []\n","    fold_indices = []\n","    for train_idx, val_idx in skf.split(dataset, targets):\n","        fold_data_sizes.append((len(train_idx), len(val_idx)))\n","        fold_indices.append((train_idx, val_idx))\n","\n","    # ✅ 실험 등록 및 fold ID 초기화\n","    experiment_id, _ = insert_experiment_with_folds(\n","        db_path=db_path,\n","        model_name=model_name,\n","        total_epochs=epochs,\n","        k_fold=k,\n","        lr=lr,\n","        patience=patience,\n","        fold_data_sizes=fold_data_sizes\n","    )\n","\n","    # fold별 모델, 옵티마이저, 데이터로더 초기화\n","    fold_models, fold_optimizers = [], []\n","    fold_train_loaders, fold_val_loaders = [], []\n","\n","    for i, (train_idx, val_idx) in enumerate(fold_indices):\n","        augmented_train_dataset = RotatedAugmentedDataset(dataset, train_idx)\n","        val_subset = torch.utils.data.Subset(dataset, val_idx)\n","\n","        print(f\"📊 Fold {i+1} - Augmented Train Dataset Size: {len(augmented_train_dataset)} (Original: {len(train_idx)}), Validation Size: {len(val_idx)}\")\n","\n","        show_augmented_images(augmented_train_dataset)\n","\n","        model = get_model(model_name, num_classes).to(device)\n","        optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","        train_loader = torch.utils.data.DataLoader(augmented_train_dataset, batch_size=4, shuffle=True)\n","        val_loader = torch.utils.data.DataLoader(val_subset, batch_size=4, shuffle=False)\n","\n","        fold_models.append(model)\n","        fold_optimizers.append(optimizer)\n","        fold_train_loaders.append(train_loader)\n","        fold_val_loaders.append(val_loader)\n","\n","    # 학습 루프\n","    best_avg_val_loss = float(\"inf\")\n","    no_improve_count = 0\n","\n","    for epoch in tqdm(range(epochs), desc=f\"[{model_name} CV-KFold]\"):\n","        fold_val_losses, fold_val_accs = [], []\n","\n","        for fold in range(k):\n","            model = fold_models[fold]\n","            optimizer = fold_optimizers[fold]\n","            train_loader = fold_train_loaders[fold]\n","            val_loader = fold_val_loaders[fold]\n","\n","            # 학습\n","            train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, device)\n","\n","            # 검증\n","            val_loss, val_acc = validate_model(model, val_loader, device, nn.CrossEntropyLoss())\n","\n","            # ✅ 로그 저장\n","            insert_epoch_log(db_path, experiment_id, fold+1, epoch+1, train_loss, val_loss, train_acc, val_acc)\n","\n","            fold_val_losses.append(val_loss)\n","            fold_val_accs.append(val_acc)\n","\n","        # Cross-Fold 평균으로 early stop 감시\n","        avg_val_loss = sum(fold_val_losses) / k\n","\n","        if avg_val_loss < best_avg_val_loss:\n","            best_avg_val_loss = avg_val_loss\n","            no_improve_count = 0\n","\n","            # ✅ fold별 모델 저장\n","            for fold in range(k):\n","                model_path = os.path.join(save_dir, f\"best_model_{model_name}_fold{fold+1}.pth\")\n","                torch.save(fold_models[fold].state_dict(), model_path)\n","\n","        else:\n","            no_improve_count += 1\n","\n","        if no_improve_count >= patience:\n","            print(f\"🛑 Early stopping at epoch {epoch+1}\")\n","            break\n","\n","    # ✅ fold별 학습 종료 후 정보 업데이트\n","    for fold in range(k):\n","        model_path = os.path.join(save_dir, f\"best_model_{model_name}_fold{fold+1}.pth\")\n","        log_plot_path = os.path.join(save_dir, f\"training_plot_{model_name}_fold{fold+1}.png\")\n","\n","        # 그래프 저장\n","        plot_log_from_db(\n","            db_path=db_path,\n","            experiment_id=experiment_id,\n","            fold=fold + 1,  # ✅ 숫자형 fold index\n","            model_name=model_name,\n","            save_path=log_plot_path\n","        )\n","\n","        # fold 정보 업데이트\n","        update_fold_info(\n","            db_path,\n","            experiment_id,\n","            fold=fold+1,\n","            best_model_path=model_path,\n","            early_stopped_epoch=epoch+1,\n","            log_plot_path=log_plot_path\n","        )\n","\n","    print(f\"\\n✅ {model_name} 모델의 CV K-Fold 학습 완료 (experiment_id: {experiment_id})\")\n","    return experiment_id\n"],"metadata":{"id":"KBWgCGSG2R5J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Zu27wrX_2XDx"},"execution_count":null,"outputs":[]}]}