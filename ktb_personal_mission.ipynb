{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPgmg9CQ6oKOCL8SHH2qqA+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"WhnvjzFCofNG"},"outputs":[],"source":["import torch\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"code","source":["# êµ¬ê¸€ ì½”ë©ì—ì„œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ì¤€ë¹„\n","from google.colab import files\n","files.upload()  # 'kaggle.json' íŒŒì¼ ì—…ë¡œë“œ\n","\n","!mkdir -p ~/.kaggle\n","!cp kaggle.json ~/.kaggle/\n","!chmod 600 ~/.kaggle/kaggle.json\n","\n","# ì•„ëª¬ë“œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ\n","!kaggle datasets download -d mahyeks/almond-varieties\n","\n","# almond í´ë”ì— ë°ì´í„° ì••ì¶• í•´ì œ\n","!unzip almond-varieties.zip -d almond"],"metadata":{"id":"mRl1ksg0pq5N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# EDA\n","\n","import os\n","import cv2\n","import random\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from collections import Counter\n","\n","# ë°ì´í„° í´ë” ê²½ë¡œ\n","root_dir = 'almond/dataset'\n","\n","# ë¼ë²¨ ëª©ë¡\n","labels = os.listdir(root_dir)\n","\n","# ê° ë¼ë²¨ë³„ ë°ì´í„° ê°œìˆ˜ í™•ì¸\n","data_counts = {}\n","example_images = {}\n","image_sizes = []\n","aspect_ratios = []\n","color_distributions = []\n","brightness_values = []\n","file_types = Counter()\n","\n","for label in labels:\n","    data_dir = os.path.join(root_dir, label)\n","    datas = os.listdir(data_dir)\n","    data_counts[label] = len(datas)\n","\n","    # ì˜ˆì‹œ ì´ë¯¸ì§€ ì €ì¥\n","    if datas:\n","        example_images[label] = os.path.join(data_dir, random.choice(datas))\n","\n","    # ì´ë¯¸ì§€ í¬ê¸° ë° ìƒ‰ìƒ ë¶„ì„\n","    for img_name in datas:\n","        img_path = os.path.join(data_dir, img_name)\n","        file_extension = os.path.splitext(img_name)[1].lower()\n","        file_types[file_extension] += 1\n","\n","        img = cv2.imread(img_path)\n","        if img is not None:\n","            h, w, _ = img.shape\n","            image_sizes.append((w, h))\n","            aspect_ratios.append(w / h)\n","\n","# ë°ì´í„° ë¶„í¬ ì‹œê°í™”\n","plt.figure(figsize=(8, 5))\n","plt.bar(data_counts.keys(), data_counts.values(), color='skyblue')\n","plt.xlabel('Labels')\n","plt.ylabel('Number of Images')\n","plt.title('Dataset Distribution')\n","plt.show()\n","\n","# ì˜ˆì‹œ ì´ë¯¸ì§€ ì‹œê°í™”\n","fig, axes = plt.subplots(1, len(example_images), figsize=(15, 5))\n","\n","for ax, (label, img_path) in zip(axes, example_images.items()):\n","    img = cv2.imread(img_path)\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    ax.imshow(img)\n","    ax.set_title(label)\n","    ax.axis('off')\n","\n","plt.show()\n","\n","# ë°ì´í„° ê°œìˆ˜ ì¶œë ¥\n","print(\"Dataset Image Count:\")\n","for label, count in data_counts.items():\n","    print(f\"{label}: {count} images\")\n","\n","# ìµœì†Œ, ìµœëŒ€ ì´ë¯¸ì§€ í¬ê¸° ì¶œë ¥\n","if image_sizes:\n","    min_size = min(image_sizes, key=lambda x: x[0] * x[1])\n","    max_size = max(image_sizes, key=lambda x: x[0] * x[1])\n","    print(f\"Smallest Image Size: {min_size[1]}x{min_size[0]}\")\n","    print(f\"Largest Image Size: {max_size[1]}x{max_size[0]}\")\n","\n","# íŒŒì¼ í˜•ì‹ ê°œìˆ˜ ì¶œë ¥\n","print(\"File Type Distribution:\")\n","for ext, count in file_types.items():\n","    print(f\"{ext}: {count} files\")\n"],"metadata":{"id":"J3ePpIF8psLF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**ì´ˆê¸° ì‹¤í—˜**"],"metadata":{"id":"pvPXVbDVuyiK"}},{"cell_type":"code","source":["# data ë¹„ìœ¨ ë³„ dataloader\n","\n","import torch\n","\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader, random_split\n","\n","def set_dataloader(split_ratio):\n","    # ë¼ë²¨ ëª©ë¡\n","    labels = os.listdir(root_dir)\n","    label_to_index = {label: idx for idx, label in enumerate(labels)}\n","    num_classes = len(labels)\n","\n","    # ì „ì²´ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ ìƒì„± (í´ë”ë³„ë¡œ ì¼ì • ë¹„ìœ¨ ìœ ì§€)\n","    data_list = {label: [] for label in labels}\n","    for label in labels:\n","        data_dir = os.path.join(root_dir, label)\n","        datas = os.listdir(data_dir)\n","        for img_name in datas:\n","            img_path = os.path.join(data_dir, img_name)\n","            data_list[label].append((img_path, label_to_index[label]))\n","\n","    # ë°ì´í„°ì…‹ ë¶„í•  (í´ë”ë³„ë¡œ ìœ ì§€)\n","    train_data, val_data, test_data = [], [], []\n","    for label, items in data_list.items():\n","        random.shuffle(items)\n","        total_size = len(items)\n","        train_end = int(split_ratio * total_size)\n","        val_end = train_end + int((split_ratio/2) * total_size)\n","\n","        train_data.extend(items[:train_end])\n","        val_data.extend(items[train_end:val_end])\n","        test_data.extend(items[val_end:])\n","\n","    # ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜\n","    class CustomImageDataset(Dataset):\n","        def __init__(self, data, transform=None):\n","            self.data = data\n","            self.transform = transform\n","\n","        def __len__(self):\n","            return len(self.data)\n","\n","        def __getitem__(self, idx):\n","            img_path, label = self.data[idx]\n","            img = cv2.imread(img_path)\n","            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","            if self.transform:\n","                img = self.transform(img)\n","\n","            return img, label\n","\n","    # ë°ì´í„° ë³€í™˜ ì •ì˜\n","    transform = transforms.Compose([\n","        transforms.ToPILImage(),\n","        transforms.Resize((224, 224)),  # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ])\n","\n","    # ë°ì´í„°ì…‹ ìƒì„±\n","    train_dataset = CustomImageDataset(train_data, transform=transform)\n","    val_dataset = CustomImageDataset(val_data, transform=transform)\n","    test_dataset = CustomImageDataset(test_data, transform=transform)\n","\n","    batch_size = 16\n","\n","    # ë°ì´í„°ë¡œë” ìƒì„±\n","    dataloader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n","    dataloader_val = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n","    dataloader_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n","\n","    # ë°ì´í„°ì…‹ í¬ê¸° ì¶œë ¥\n","    print(f\"Training Dataset Size: {len(train_dataset)}\")\n","    print(f\"Validation Dataset Size: {len(val_dataset)}\")\n","    print(f\"Test Dataset Size: {len(test_dataset)}\")\n","\n","    # ë°ì´í„° í™•ì¸\n","    for images, labels in dataloader_train:\n","        print(f\"Train Batch Image Shape: {images.shape}\")\n","        print(f\"Train Batch Labels: {labels}\")\n","        break\n","\n","    return dataloader_train, dataloader_val, dataloader_test, num_classes"],"metadata":{"id":"JEoI82ZpqMuf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ëª¨ë¸ ì •ì˜ í•¨ìˆ˜\n","\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.models as models\n","\n","def get_model(model_name, num_classes):\n","    if model_name == 'resnet50':\n","        model = models.resnet50(pretrained=True)\n","        model.fc = nn.Linear(model.fc.in_features, num_classes)\n","    elif model_name == 'efficientnet_b0':\n","        model = models.efficientnet_b0(pretrained=True)\n","        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n","    elif model_name == 'mobilenet_v3_small':\n","        model = models.mobilenet_v3_small(pretrained=True)\n","        model.classifier[3] = nn.Linear(model.classifier[3].in_features, num_classes)\n","    elif model_name == 'shufflenet_v2_x1_0':\n","        model = models.shufflenet_v2_x1_0(pretrained=True)\n","        model.fc = nn.Linear(model.fc.in_features, num_classes)\n","    else:\n","        raise ValueError(\"Unsupported model\")\n","    return model"],"metadata":{"id":"RdQIFCxmq-6G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# í•™ìŠµ í•¨ìˆ˜\n","def train_model(model_name, train_loader, val_loader, num_classes, save_dir, device, split_ratio, epochs=10, lr=0.001, patience=3):\n","    model = get_model(model_name, num_classes=num_classes)\n","    model = model.to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    best_val_loss = float('inf')\n","    no_improve_count = 0\n","    log_file = open(f\"{save_dir}/training_log_{model_name}.txt\", \"w\")\n","    log_file.write(f\"Model: {model_name}, Epochs: {epochs}, Learning Rate: {lr}, Patience: {patience}, Train Ratio: {split_ratio*100}%, Validation Ratio: {split_ratio*50}%\\n\")\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","\n","        for images, labels in train_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","        train_acc = correct / total\n","        val_loss, val_acc = validate_model(model, val_loader, device, criterion)\n","        log_file.write(f\"Epoch {epoch+1}/{epochs}, Train Loss: {running_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\\n\")\n","        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {running_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            no_improve_count = 0\n","            torch.save(model.state_dict(), f\"{save_dir}/best_model_{model_name}.pth\")\n","        else:\n","            no_improve_count += 1\n","\n","        if no_improve_count >= patience:\n","            print(f\"Early stopping at epoch {epoch+1} due to no improvement in validation accuracy.\")\n","            log_file.write(f\"Early stopping at epoch {epoch+1}\\n\")\n","            break\n","\n","    log_file.close()\n","    print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n","\n","# validation í‰ê°€ í•¨ìˆ˜ (Lossì™€ Accuracy ë°˜í™˜)\n","def validate_model(model, dataloader, device, criterion):\n","    model.eval()\n","    total_loss = 0.0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for images, labels in dataloader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item()\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    avg_loss = total_loss / len(dataloader)\n","    accuracy = correct / total\n","    return avg_loss, accuracy"],"metadata":{"id":"H8zqwu0UrA8Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_next_directory(base_path):\n","    # base_path ê²½ë¡œ ë‚´ì˜ ê¸°ì¡´ ë””ë ‰í† ë¦¬ ëª©ë¡ í™•ì¸\n","    existing_dirs = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]\n","\n","    # ìˆ«ìë¡œ ëœ ë””ë ‰í† ë¦¬ë§Œ í•„í„°ë§\n","    numbered_dirs = sorted([int(d) for d in existing_dirs if d.isdigit()])\n","\n","    # ë‹¤ìŒ ë””ë ‰í† ë¦¬ ë²ˆí˜¸ ê²°ì •\n","    next_number = numbered_dirs[-1] + 1 if numbered_dirs else 1\n","    new_dir = os.path.join(base_path, str(next_number))\n","\n","    # ë””ë ‰í† ë¦¬ ìƒì„±\n","    os.makedirs(new_dir, exist_ok=True)\n","    print(f\"Created directory: {new_dir}\")\n","\n","    return new_dir"],"metadata":{"id":"9NKRpZARrF0n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# í‰ê°€ í•¨ìˆ˜\n","import seaborn as sns\n","\n","from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n","\n","# Fine-tuned ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜\n","def load_fine_tuned_model(model_name, num_classes, checkpoint_path, device):\n","    if model_name == 'resnet50':\n","        model = models.resnet50(pretrained=False)\n","        model.fc = nn.Linear(model.fc.in_features, num_classes)\n","    elif model_name == 'efficientnet_b0':\n","        model = models.efficientnet_b0(pretrained=False)\n","        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n","    elif model_name == 'mobilenet_v3_small':\n","        model = models.mobilenet_v3_small(pretrained=True)\n","        model.classifier[3] = nn.Linear(model.classifier[3].in_features, num_classes)\n","    elif model_name == 'shufflenet_v2_x1_0':\n","        model = models.shufflenet_v2_x1_0(pretrained=True)\n","        model.fc = nn.Linear(model.fc.in_features, num_classes)\n","    else:\n","        raise ValueError(\"Unsupported model\")\n","\n","    model.load_state_dict(torch.load(checkpoint_path))\n","    model.to(device)\n","    return model\n","\n","\n","# Confusion Matrix ì‹œê°í™” í•¨ìˆ˜\n","def plot_confusion_matrix(y_true, y_pred, classes, model_name):\n","    cm = confusion_matrix(y_true, y_pred)\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n","    plt.xlabel('Predicted Label')\n","    plt.ylabel('True Label')\n","    plt.title(f'{model_name} Confusion Matrix')\n","    plt.show()\n","\n","# ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë° Confusion Matrix ì¶œë ¥\n","def evaluate_model(model_name, dir_num, dataloader, device):\n","    checkpoint_path = os.path.join(dir_num, f\"best_model_{model_name}.pth\")\n","    model = load_fine_tuned_model(model_name, num_classes, checkpoint_path, device)\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","    with torch.no_grad():\n","        for images, labels in dataloader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, preds = torch.max(outputs, 1)\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    # ë°ì´í„° í´ë” ê²½ë¡œ\n","    root_dir = 'almond/dataset'\n","\n","    # ë¼ë²¨ ëª©ë¡\n","    labels = os.listdir(root_dir)\n","\n","    # í‰ê°€ ì§€í‘œ ê³„ì‚°\n","    accuracy = accuracy_score(all_labels, all_preds)\n","    precision = precision_score(all_labels, all_preds, average='weighted')\n","    recall = recall_score(all_labels, all_preds, average='weighted')\n","    f1 = f1_score(all_labels, all_preds, average='weighted')\n","\n","    print(f\"Test Accuracy: {accuracy:.4f}\")\n","    print(f\"Test Precision: {precision:.4f}\")\n","    print(f\"Test Recall: {recall:.4f}\")\n","    print(f\"Test F1 Score: {f1:.4f}\")\n","\n","    plot_confusion_matrix(all_labels, all_preds, labels, model_name)\n","\n","# í‰ê°€ í•¨ìˆ˜\n","import os\n","import torch\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","from torch import nn\n","from torchvision import models\n","from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n","\n","def process_single_experiment(dataloader, device, num_classes, experiment_dir):\n","    \"\"\"\n","    íŠ¹ì • ì‹¤í—˜ í´ë”(ì˜ˆ: /content/drive/MyDrive/KTB/personal_mission2/train_dir/1)ì—ì„œë§Œ\n","    í•™ìŠµ ë¡œê·¸ ë° Confusion Matrixë¥¼ ì €ì¥í•˜ëŠ” í•¨ìˆ˜\n","    \"\"\"\n","\n","    # train_dirì—ì„œ test_dirë¡œ ë³€í™˜\n","    test_experiment_dir = experiment_dir.replace(\"train_dir\", \"test_dir\")\n","\n","    # experiment_dir ì¡´ì¬ ì—¬ë¶€ í™•ì¸\n","    if not os.path.exists(experiment_dir):\n","        print(f\"âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ ì‹¤í—˜ ë””ë ‰í† ë¦¬: {experiment_dir}\")\n","        return\n","\n","    # í•´ë‹¹ ì‹¤í—˜ í´ë” ì•ˆì˜ training_log íŒŒì¼ ì°¾ê¸°\n","    txt_files = [os.path.join(experiment_dir, file) for file in os.listdir(experiment_dir) if file.startswith(\"training_log_\") and file.endswith(\".txt\")]\n","\n","    if not txt_files:\n","        print(f\"âŒ {experiment_dir}ì—ì„œ training_log íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n","        return\n","\n","    os.makedirs(test_experiment_dir, exist_ok=True)  # test_dirì— í•´ë‹¹ ì‹¤í—˜ í´ë” ìƒì„±\n","\n","    for txt_file in txt_files:\n","        df = parse_log(txt_file)\n","        if df is not None:\n","            model_name = extract_model_name(os.path.basename(txt_file))\n","\n","            # í•™ìŠµ ë¡œê·¸ ê·¸ë˜í”„ ì €ì¥\n","            plt.figure(figsize=(12, 5))\n","\n","            # Loss ê·¸ë˜í”„\n","            plt.subplot(1, 2, 1)\n","            plt.plot(df[\"Epoch\"], df[\"Train Loss\"], label=\"Train Loss\", marker=\"o\")\n","            plt.plot(df[\"Epoch\"], df[\"Val Loss\"], label=\"Val Loss\", marker=\"s\")\n","            plt.xlabel(\"Epoch\")\n","            plt.ylabel(\"Loss\")\n","            plt.title(f\"Training & Validation Loss ({model_name})\")\n","            plt.legend()\n","            plt.grid(True)\n","\n","            # Accuracy ê·¸ë˜í”„\n","            plt.subplot(1, 2, 2)\n","            plt.plot(df[\"Epoch\"], df[\"Train Acc\"], label=\"Train Acc\", marker=\"o\")\n","            plt.plot(df[\"Epoch\"], df[\"Val Acc\"], label=\"Val Acc\", marker=\"s\")\n","            plt.xlabel(\"Epoch\")\n","            plt.ylabel(\"Accuracy\")\n","            plt.title(f\"Training & Validation Accuracy ({model_name})\")\n","            plt.legend()\n","            plt.grid(True)\n","\n","            plt.tight_layout()\n","            plt.savefig(os.path.join(test_experiment_dir, f\"training_log_{model_name}.png\"))\n","            plt.close()\n","\n","            # Confusion Matrix ë° ì„±ëŠ¥ ì§€í‘œ ì €ì¥\n","            evaluate_model(model_name, experiment_dir, dataloader, device, num_classes)\n","\n","    print(f\"âœ… {experiment_dir}ì˜ í•™ìŠµ ë¡œê·¸ ë° Confusion Matrixê°€ {test_experiment_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n","\n"],"metadata":{"id":"DzfHpvNArHl8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for ratio in [0.8, 0.7, 0.6, 0.5, 0.4]:\n","    dataloader_train, dataloader_val, dataloader_test, num_classes = set_dataloader(ratio)\n","\n","    save_dir = create_next_directory('/content/drive/MyDrive/KTB/personal_mission/train_dir')\n","    for model_name in ['resnet50', 'efficientnet_b0', 'mobilenet_v3_small', 'shufflenet_v2_x1_0']:\n","        print(f\"Training {model_name}...\")\n","        train_model(model_name, dataloader_train, dataloader_val, num_classes, save_dir, device, ratio, epochs=100, lr=0.001, patience=10)\n","    process_single_experiment(dataloader_test, device, num_classes, save_dir)"],"metadata":{"id":"FLDo4IjnsJ6b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for ratio in [0.3, 0.2, 0.1, 0.08, 0.06, 0.04, 0.03, 0.02, 0.01]:\n","    dataloader_train, dataloader_val, dataloader_test, num_classes = set_dataloader(ratio)\n","\n","    save_dir = create_next_directory('/content/drive/MyDrive/KTB/personal_mission/train_dir')\n","    for model_name in ['resnet50', 'efficientnet_b0', 'mobilenet_v3_small', 'shufflenet_v2_x1_0']:\n","        print(f\"Training {model_name}...\")\n","        train_model(model_name, dataloader_train, dataloader_val, num_classes, save_dir, device, ratio, epochs=100, lr=0.0001, patience=10)\n","    process_single_experiment(dataloader_test, device, num_classes, save_dir)"],"metadata":{"id":"SQidgP6_rjzU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë° Confusion Matrix ì €ì¥\n","def evaluate_baseline_model(model_name, dir_path, dataloader, device, num_classes):\n","    model = get_model(model_name, num_classes)\n","    model.to(device)\n","    if model != None:\n","        model.eval()\n","        all_preds, all_labels = [], []\n","\n","        with torch.no_grad():\n","            for images, labels in dataloader:\n","                images, labels = images.to(device), labels.to(device)\n","                outputs = model(images)\n","                _, preds = torch.max(outputs, 1)\n","                all_preds.extend(preds.cpu().numpy())\n","                all_labels.extend(labels.cpu().numpy())\n","\n","        # ë°ì´í„°ì…‹ì˜ í´ë˜ìŠ¤ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°\n","        root_dir = 'almond/dataset'\n","        classes = os.listdir(root_dir)\n","\n","        # í‰ê°€ ì§€í‘œ ê³„ì‚°\n","        accuracy = accuracy_score(all_labels, all_preds)\n","        precision = precision_score(all_labels, all_preds, average='weighted')\n","        recall = recall_score(all_labels, all_preds, average='weighted')\n","        f1 = f1_score(all_labels, all_preds, average='weighted')\n","\n","        print(f\"Test Accuracy: {accuracy:.4f}\")\n","        print(f\"Test Precision: {precision:.4f}\")\n","        print(f\"Test Recall: {recall:.4f}\")\n","        print(f\"Test F1 Score: {f1:.4f}\")\n","\n","        # ì €ì¥ ê²½ë¡œ ë³€í™˜ (train_dir -> test_dir)\n","        save_path = dir_path.replace(train_root, test_root)\n","        os.makedirs(save_path, exist_ok=True)\n","        cm_save_path = os.path.join(save_path, f\"confusion_matrix_{model_name}.png\")\n","\n","        plot_confusion_matrix(all_labels, all_preds, classes, model_name, cm_save_path)\n","\n","        # í‰ê°€ì§€í‘œ ì €ì¥\n","        metrics_save_path = os.path.join(save_path, f\"metrics_{model_name}.txt\")\n","        with open(metrics_save_path, \"w\") as f:\n","            f.write(f\"Model: {model_name}\\n\")\n","            f.write(f\"Test Accuracy: {accuracy:.4f}\\n\")\n","            f.write(f\"Test Precision: {precision:.4f}\\n\")\n","            f.write(f\"Test Recall: {recall:.4f}\\n\")\n","            f.write(f\"Test F1 Score: {f1:.4f}\\n\")\n","\n","        print(f\"ğŸ“„ {metrics_save_path}ì— í‰ê°€ì§€í‘œ ì €ì¥ ì™„ë£Œ!\")"],"metadata":{"id":"O4gKyqbpuAG1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for model_name, ratio in zip(['resnet50', 'efficientnet_b0', 'mobilenet_v3_small', 'shufflenet_v2_x1_0'], [0.01, 0.02, 0.08, 0.03]):\n","    dataloader_train, dataloader_val, dataloader_test, num_classes = set_dataloader(ratio)\n","\n","    save_dir = '/content/drive/MyDrive/KTB/personal_mission/baseline'\n","\n","    evaluate_baseline_model(model_name, save_dir, dataloader_test, device, num_classes)"],"metadata":{"id":"AWl_4IGqt9Xc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**í›„ê¸° ì‹¤í—˜**"],"metadata":{"id":"x7pMnug2usg6"}},{"cell_type":"code","source":["# ì†Œìˆ˜ì˜ datasetìœ¼ë¡œ êµ¬ì„±ëœ dataloader\n","\n","import torch\n","\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader, random_split\n","\n","def set_dataloader():\n","    # ë¼ë²¨ ëª©ë¡\n","    labels = os.listdir(root_dir)\n","    label_to_index = {label: idx for idx, label in enumerate(labels)}\n","    num_classes = len(labels)\n","\n","    # ì „ì²´ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ ìƒì„± (í´ë”ë³„ë¡œ ì¼ì • ë¹„ìœ¨ ìœ ì§€)\n","    data_list = {label: [] for label in labels}\n","    for label in labels:\n","        data_dir = os.path.join(root_dir, label)\n","        datas = os.listdir(data_dir)\n","        for img_name in datas:\n","            img_path = os.path.join(data_dir, img_name)\n","            data_list[label].append((img_path, label_to_index[label]))\n","\n","    # ë°ì´í„°ì…‹ ë¶„í•  (í´ë”ë³„ë¡œ ìœ ì§€)\n","    train_data, val_data, test_data = [], [], []\n","    for label, items in data_list.items():\n","        random.shuffle(items)\n","        total_size = len(items)\n","\n","        train_data.extend(items[:4])\n","        val_data.extend(items[4:6])\n","        test_data.extend(items[6:])\n","\n","    # ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜\n","    class CustomImageDataset(Dataset):\n","        def __init__(self, data, transform=None):\n","            self.data = data\n","            self.transform = transform\n","\n","        def __len__(self):\n","            return len(self.data)\n","\n","        def __getitem__(self, idx):\n","            img_path, label = self.data[idx]\n","            img = cv2.imread(img_path)\n","            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","            if self.transform:\n","                img = self.transform(img)\n","\n","            return img, label\n","\n","    # ë°ì´í„° ë³€í™˜ ì •ì˜\n","    transform = transforms.Compose([\n","        transforms.ToPILImage(),\n","        transforms.Resize((224, 224)),  # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ])\n","\n","    # ë°ì´í„°ì…‹ ìƒì„±\n","    train_dataset = CustomImageDataset(train_data, transform=transform)\n","    val_dataset = CustomImageDataset(val_data, transform=transform)\n","    test_dataset = CustomImageDataset(test_data, transform=transform)\n","\n","    batch_size = 4\n","\n","    # ë°ì´í„°ë¡œë” ìƒì„±\n","    dataloader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n","    dataloader_val = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n","    dataloader_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n","\n","    # ë°ì´í„°ì…‹ í¬ê¸° ì¶œë ¥\n","    print(f\"Training Dataset Size: {len(train_dataset)}\")\n","    print(f\"Validation Dataset Size: {len(val_dataset)}\")\n","    print(f\"Test Dataset Size: {len(test_dataset)}\")\n","\n","    # ë°ì´í„° í™•ì¸\n","    for images, labels in dataloader_train:\n","        print(f\"Train Batch Image Shape: {images.shape}\")\n","        print(f\"Train Batch Labels: {labels}\")\n","        break\n","\n","    return dataloader_train, dataloader_val, dataloader_test, num_classes"],"metadata":{"id":"UUKNwpEwuvxy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# augmentation dataloader\n","\n","import torch\n","\n","from PIL import Image\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader, random_split\n","\n","def set_aug_dataloader():\n","    # ë¼ë²¨ ëª©ë¡\n","    labels = os.listdir(root_dir)\n","    label_to_index = {label: idx for idx, label in enumerate(labels)}\n","    num_classes = len(labels)\n","\n","    # ì „ì²´ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ ìƒì„± (í´ë”ë³„ë¡œ ì¼ì • ë¹„ìœ¨ ìœ ì§€)\n","    data_list = {label: [] for label in labels}\n","    for label in labels:\n","        data_dir = os.path.join(root_dir, label)\n","        datas = os.listdir(data_dir)\n","        for img_name in datas:\n","            img_path = os.path.join(data_dir, img_name)\n","            data_list[label].append((img_path, label_to_index[label]))\n","\n","    # ë°ì´í„°ì…‹ ë¶„í•  (í´ë”ë³„ë¡œ ìœ ì§€)\n","    train_data, val_data, test_data = [], [], []\n","    for label, items in data_list.items():\n","        random.shuffle(items)\n","        total_size = len(items)\n","\n","        train_data.extend(items[:4])\n","        val_data.extend(items[4:6])\n","        test_data.extend(items[6:])\n","\n","    # image_list, label_list ë¶„ë¦¬\n","    train_image_list = []\n","    train_label_list = []\n","\n","    for img_path, label in train_data:\n","        image = Image.open(img_path).convert('RGB')  # ì´ë¯¸ì§€ ë¡œë”© ë° RGB ë³€í™˜\n","        train_image_list.append(image)\n","        train_label_list.append(label)\n","\n","    # Augmentation í¬í•¨ ë°ì´í„°ì…‹ í´ë˜ìŠ¤\n","    class RotatedAugmentedDataset(Dataset):\n","        def __init__(self, image_list, label_list):\n","            self.image_list = image_list\n","            self.label_list = label_list\n","\n","            # ê³µí†µ transform (í¬ê¸° ì¡°ì • + ì •ê·œí™”)\n","            self.base_transform = transforms.Compose([\n","                transforms.ToTensor(),\n","                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                    std=[0.229, 0.224, 0.225])\n","            ])\n","\n","            # íšŒì „ ê°ë„\n","            self.angles = [0, 90, 180, 270]\n","\n","        def __len__(self):\n","            # ë°ì´í„° 4ë°°ë¡œ ë°˜í™˜\n","            return len(self.image_list) * 4\n","\n","        def __getitem__(self, idx):\n","            img_idx = idx // 4\n","            angle_idx = idx % 4\n","\n","            image = self.image_list[img_idx]\n","            label = self.label_list[img_idx]\n","\n","            # Resize ë¨¼ì €\n","            resized_image = image.resize((224, 224))\n","\n","            # íšŒì „ ì ìš©\n","            rotated_image = resized_image.rotate(self.angles[angle_idx])\n","\n","            # ì •ê·œí™” ë° í…ì„œ ë³€í™˜\n","            transformed_image = self.base_transform(rotated_image)\n","\n","            return transformed_image, label\n","\n","    # ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜\n","    class CustomImageDataset(Dataset):\n","        def __init__(self, data, transform=None):\n","            self.data = data\n","            self.transform = transform\n","\n","        def __len__(self):\n","            return len(self.data)\n","\n","        def __getitem__(self, idx):\n","            img_path, label = self.data[idx]\n","            img = cv2.imread(img_path)\n","            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","            if self.transform:\n","                img = self.transform(img)\n","\n","            return img, label\n","\n","    # ë°ì´í„° ë³€í™˜ ì •ì˜\n","    transform = transforms.Compose([\n","        transforms.ToPILImage(),\n","        transforms.Resize((224, 224)),  # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ])\n","\n","    # ë°ì´í„°ì…‹ ìƒì„±\n","    train_dataset = RotatedAugmentedDataset(train_image_list, train_label_list)\n","    val_dataset = CustomImageDataset(val_data, transform=transform)\n","    test_dataset = CustomImageDataset(test_data, transform=transform)\n","\n","    batch_size = 4\n","\n","    # ë°ì´í„°ë¡œë” ìƒì„±\n","    dataloader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n","    dataloader_val = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n","    dataloader_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n","\n","    # ë°ì´í„°ì…‹ í¬ê¸° ì¶œë ¥\n","    print(f\"Training Dataset Size: {len(train_dataset)}\")\n","    print(f\"Validation Dataset Size: {len(val_dataset)}\")\n","    print(f\"Test Dataset Size: {len(test_dataset)}\")\n","\n","    # ë°ì´í„° í™•ì¸\n","    for images, labels in dataloader_train:\n","        print(f\"Train Batch Image Shape: {images.shape}\")\n","        print(f\"Train Batch Labels: {labels}\")\n","        break\n","\n","    return dataloader_train, dataloader_val, dataloader_test, num_classes"],"metadata":{"id":"wPoip19qvkp-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ëª¨ë¸ ì •ì˜ í•¨ìˆ˜ (Deep CNN ë²„ì „)\n","\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.models as models\n","\n","def get_model(model_name, num_classes):\n","    if model_name == 'resnet50':\n","        model = models.resnet50(pretrained=True)\n","        model.fc = nn.Linear(model.fc.in_features, num_classes)\n","    elif model_name == 'resnet152':\n","        model = models.resnet152(pretrained=True)\n","        model.fc = nn.Linear(model.fc.in_features, num_classes)\n","    elif model_name == 'vgg16_bn':\n","        model = models.vgg16_bn(pretrained=True)\n","        model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n","    elif model_name == 'densenet201':\n","        model = models.densenet201(pretrained=True)\n","        model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n","    elif model_name == 'seresnext101':\n","        model = models.resnext101_32x8d(pretrained=True)  # SE-ResNeXtëŠ” torchvisionì— ì—†ìŒ â†’ ìœ ì‚¬í•œ êµ¬ì¡°ë¡œ ëŒ€ì²´\n","        model.fc = nn.Linear(model.fc.in_features, num_classes)\n","    elif model_name == 'convnext_base':\n","        model = models.convnext_base(pretrained=True)\n","        model.classifier[2] = nn.Linear(model.classifier[2].in_features, num_classes)\n","    else:\n","        raise ValueError(f\"Unsupported model: {model_name}\")\n","\n","    return model\n"],"metadata":{"id":"4L64Dz1OvBCv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# í‰ê°€ í•¨ìˆ˜(í•™ìŠµ í•¨ìˆ˜ëŠ” ì´ˆê¸° ì‹¤í—˜ í•¨ìˆ˜ ì‚¬ìš©)\n","\n","import os\n","import re\n","import torch\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","from torch import nn\n","from torchvision import models\n","from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n","\n","# Root ë””ë ‰í† ë¦¬ ì„¤ì •\n","train_root = \"/content/drive/MyDrive/KTB/personal_mission/train_dir\"\n","test_root = \"/content/drive/MyDrive/KTB/personal_mission/test_dir\"\n","\n","# ëª¨ë“  txt íŒŒì¼ì„ ì°¾ëŠ” í•¨ìˆ˜\n","def find_txt_files(root_dir):\n","    txt_files = []\n","    for subdir, _, files in os.walk(root_dir):\n","        for file in files:\n","            if file.startswith(\"training_log_\") and file.endswith(\".txt\"):\n","                txt_files.append(os.path.join(subdir, file))\n","    return txt_files\n","\n","# txt íŒŒì¼ì—ì„œ í•™ìŠµ ë¡œê·¸ë¥¼ ì½ê³  íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜\n","def parse_log(file_path):\n","    epochs, train_loss, val_loss, train_acc, val_acc = [], [], [], [], []\n","\n","    with open(file_path, \"r\") as f:\n","        for line in f:\n","            match = re.search(\n","                r\"Epoch (\\d+)/\\d+, Train Loss: ([\\d.]+), Val Loss: ([\\d.]+), Train Acc: ([\\d.]+), Val Acc: ([\\d.]+)\", line\n","            )\n","            if match:\n","                epoch, t_loss, v_loss, t_acc, v_acc = map(float, match.groups())\n","                epochs.append(int(epoch))\n","                train_loss.append(t_loss)\n","                val_loss.append(v_loss)\n","                train_acc.append(t_acc)\n","                val_acc.append(v_acc)\n","\n","    if not epochs:  # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ None ë°˜í™˜\n","        return None\n","\n","    return pd.DataFrame({\n","        \"Epoch\": epochs,\n","        \"Train Loss\": train_loss,\n","        \"Val Loss\": val_loss,\n","        \"Train Acc\": train_acc,\n","        \"Val Acc\": val_acc\n","    })\n","\n","# íŒŒì¼ëª…ì—ì„œ ëª¨ë¸ ì´ë¦„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜\n","def extract_model_name(file_name):\n","    match = re.search(r\"training_log_(.+)\\.txt\", file_name)\n","    return match.group(1) if match else \"Unknown Model\"\n","\n","# Fine-tuned ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜\n","def load_fine_tuned_model(model_name, num_classes, checkpoint_path, device):\n","    try:\n","        if model_name == 'resnet50':\n","            model = models.resnet50(pretrained=True)\n","            model.fc = nn.Linear(model.fc.in_features, num_classes)\n","        elif model_name == 'resnet152':\n","            model = models.resnet152(pretrained=True)\n","            model.fc = nn.Linear(model.fc.in_features, num_classes)\n","        elif model_name == 'vgg16_bn':\n","            model = models.vgg16_bn(pretrained=True)\n","            model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n","        elif model_name == 'densenet201':\n","            model = models.densenet201(pretrained=True)\n","            model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n","        elif model_name == 'seresnext101':\n","            model = models.resnext101_32x8d(pretrained=True)  # SE-ResNeXtëŠ” torchvisionì— ì—†ìŒ â†’ ìœ ì‚¬í•œ êµ¬ì¡°ë¡œ ëŒ€ì²´\n","            model.fc = nn.Linear(model.fc.in_features, num_classes)\n","        elif model_name == 'convnext_base':\n","            model = models.convnext_base(pretrained=True)\n","            model.classifier[2] = nn.Linear(model.classifier[2].in_features, num_classes)\n","        else:\n","            raise ValueError(f\"Unsupported model: {model_name}\")\n","\n","        model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n","        model.to(device)\n","        return model\n","\n","    except Exception as e:\n","        print(f\"âš ï¸ ëª¨ë¸ {model_name} ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n","        return None  # ì˜¤ë¥˜ ë°œìƒ ì‹œ None ë°˜í™˜\n","\n","# Confusion Matrix ì‹œê°í™” ë° ì €ì¥\n","def plot_confusion_matrix(y_true, y_pred, classes, model_name, save_path):\n","    cm = confusion_matrix(y_true, y_pred)\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n","    plt.xlabel('Predicted Label')\n","    plt.ylabel('True Label')\n","    plt.title(f'Confusion Matrix ({model_name})')\n","    plt.savefig(save_path)\n","    plt.close()\n","\n","# ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë° Confusion Matrix ì €ì¥\n","def evaluate_model(model_name, dir_path, dataloader, device, num_classes):\n","    # Root ë””ë ‰í† ë¦¬ ì„¤ì •\n","    train_root = \"/content/drive/MyDrive/KTB/personal_mission/train_dir\"\n","    test_root = \"/content/drive/MyDrive/KTB/personal_mission/test_dir\"\n","\n","    checkpoint_path = os.path.join(dir_path, f\"best_model_{model_name}.pth\")\n","    model = load_fine_tuned_model(model_name, num_classes, checkpoint_path, device)\n","    if model != None:\n","        model.eval()\n","        all_preds, all_labels = [], []\n","\n","        with torch.no_grad():\n","            for images, labels in dataloader:\n","                images, labels = images.to(device), labels.to(device)\n","                outputs = model(images)\n","                _, preds = torch.max(outputs, 1)\n","                all_preds.extend(preds.cpu().numpy())\n","                all_labels.extend(labels.cpu().numpy())\n","\n","        # ë°ì´í„°ì…‹ì˜ í´ë˜ìŠ¤ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°\n","        root_dir = 'almond/dataset'\n","        classes = os.listdir(root_dir)\n","\n","        # í‰ê°€ ì§€í‘œ ê³„ì‚°\n","        accuracy = accuracy_score(all_labels, all_preds)\n","        precision = precision_score(all_labels, all_preds, average='weighted')\n","        recall = recall_score(all_labels, all_preds, average='weighted')\n","        f1 = f1_score(all_labels, all_preds, average='weighted')\n","\n","        print(f\"Test Accuracy: {accuracy:.4f}\")\n","        print(f\"Test Precision: {precision:.4f}\")\n","        print(f\"Test Recall: {recall:.4f}\")\n","        print(f\"Test F1 Score: {f1:.4f}\")\n","\n","        # ì €ì¥ ê²½ë¡œ ë³€í™˜ (train_dir -> test_dir)\n","        save_path = dir_path.replace(train_root, test_root)\n","        os.makedirs(save_path, exist_ok=True)\n","        cm_save_path = os.path.join(save_path, f\"confusion_matrix_{model_name}.png\")\n","\n","        plot_confusion_matrix(all_labels, all_preds, classes, model_name, cm_save_path)\n","\n","        # í‰ê°€ì§€í‘œ ì €ì¥\n","        metrics_save_path = os.path.join(save_path, f\"metrics_{model_name}.txt\")\n","        with open(metrics_save_path, \"w\") as f:\n","            f.write(f\"Model: {model_name}\\n\")\n","            f.write(f\"Test Accuracy: {accuracy:.4f}\\n\")\n","            f.write(f\"Test Precision: {precision:.4f}\\n\")\n","            f.write(f\"Test Recall: {recall:.4f}\\n\")\n","            f.write(f\"Test F1 Score: {f1:.4f}\\n\")\n","\n","        print(f\"ğŸ“„ {metrics_save_path}ì— í‰ê°€ì§€í‘œ ì €ì¥ ì™„ë£Œ!\")\n","\n","# í•™ìŠµ ë¡œê·¸ + Confusion Matrix í†µí•© ì €ì¥\n","def process_single_experiment(dataloader, device, num_classes, experiment_dir):\n","    \"\"\"\n","    íŠ¹ì • ì‹¤í—˜ í´ë”(ì˜ˆ: /content/drive/MyDrive/KTB/personal_mission2/train_dir/1)ì—ì„œë§Œ\n","    í•™ìŠµ ë¡œê·¸ ë° Confusion Matrixë¥¼ ì €ì¥í•˜ëŠ” í•¨ìˆ˜\n","    \"\"\"\n","\n","    # train_dirì—ì„œ test_dirë¡œ ë³€í™˜\n","    test_experiment_dir = experiment_dir.replace(\"train_dir\", \"test_dir\")\n","\n","    # experiment_dir ì¡´ì¬ ì—¬ë¶€ í™•ì¸\n","    if not os.path.exists(experiment_dir):\n","        print(f\"âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ ì‹¤í—˜ ë””ë ‰í† ë¦¬: {experiment_dir}\")\n","        return\n","\n","    # í•´ë‹¹ ì‹¤í—˜ í´ë” ì•ˆì˜ training_log íŒŒì¼ ì°¾ê¸°\n","    txt_files = [os.path.join(experiment_dir, file) for file in os.listdir(experiment_dir) if file.startswith(\"training_log_\") and file.endswith(\".txt\")]\n","\n","    if not txt_files:\n","        print(f\"âŒ {experiment_dir}ì—ì„œ training_log íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n","        return\n","\n","    os.makedirs(test_experiment_dir, exist_ok=True)  # test_dirì— í•´ë‹¹ ì‹¤í—˜ í´ë” ìƒì„±\n","\n","    for txt_file in txt_files:\n","        df = parse_log(txt_file)\n","        if df is not None:\n","            model_name = extract_model_name(os.path.basename(txt_file))\n","\n","            # í•™ìŠµ ë¡œê·¸ ê·¸ë˜í”„ ì €ì¥\n","            plt.figure(figsize=(12, 5))\n","\n","            # Loss ê·¸ë˜í”„\n","            plt.subplot(1, 2, 1)\n","            plt.plot(df[\"Epoch\"], df[\"Train Loss\"], label=\"Train Loss\", marker=\"o\")\n","            plt.plot(df[\"Epoch\"], df[\"Val Loss\"], label=\"Val Loss\", marker=\"s\")\n","            plt.xlabel(\"Epoch\")\n","            plt.ylabel(\"Loss\")\n","            plt.title(f\"Training & Validation Loss ({model_name})\")\n","            plt.legend()\n","            plt.grid(True)\n","\n","            # Accuracy ê·¸ë˜í”„\n","            plt.subplot(1, 2, 2)\n","            plt.plot(df[\"Epoch\"], df[\"Train Acc\"], label=\"Train Acc\", marker=\"o\")\n","            plt.plot(df[\"Epoch\"], df[\"Val Acc\"], label=\"Val Acc\", marker=\"s\")\n","            plt.xlabel(\"Epoch\")\n","            plt.ylabel(\"Accuracy\")\n","            plt.title(f\"Training & Validation Accuracy ({model_name})\")\n","            plt.legend()\n","            plt.grid(True)\n","\n","            plt.tight_layout()\n","            plt.savefig(os.path.join(test_experiment_dir, f\"training_log_{model_name}.png\"))\n","            plt.close()\n","\n","            # Confusion Matrix ë° ì„±ëŠ¥ ì§€í‘œ ì €ì¥\n","            evaluate_model(model_name, experiment_dir, dataloader, device, num_classes)\n","\n","    print(f\"âœ… {experiment_dir}ì˜ í•™ìŠµ ë¡œê·¸ ë° Confusion Matrixê°€ {test_experiment_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n","\n"],"metadata":{"id":"WeD7Z-YEw9mc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Cross Validation K-Fold(DB ì—°ë™)**"],"metadata":{"id":"CkQ0iF7a1Q4V"}},{"cell_type":"code","source":["# CV K-Fold DB Table, DB Update í•¨ìˆ˜\n","\n","import sqlite3\n","from datetime import datetime\n","\n","def create_experiment_db(db_path=\"/content/drive/MyDrive/KTB/personal_mission/experiment_log.db\"):\n","    conn = sqlite3.connect(db_path)\n","    cursor = conn.cursor()\n","\n","    # 1. experiments: ì „ì²´ ì‹¤í—˜ ì •ë³´\n","    cursor.execute(\"\"\"\n","    CREATE TABLE IF NOT EXISTS experiments (\n","        id INTEGER PRIMARY KEY AUTOINCREMENT,\n","        model_name TEXT,\n","        total_epochs INTEGER,\n","        k_fold INTEGER,\n","        lr REAL,\n","        patience INTEGER,\n","        created_at TEXT\n","    )\n","    \"\"\")\n","\n","    # 2. experiment_folds: ê° foldë³„ ì •ë³´\n","    cursor.execute(\"\"\"\n","    CREATE TABLE IF NOT EXISTS experiment_folds (\n","        id INTEGER PRIMARY KEY AUTOINCREMENT,\n","        experiment_id INTEGER,\n","        fold INTEGER,\n","        train_size INTEGER,\n","        val_size INTEGER,\n","        best_model_path TEXT,\n","        early_stopped_epoch INTEGER,\n","        log_plot_path TEXT,\n","        FOREIGN KEY (experiment_id) REFERENCES experiments (id)\n","    )\n","    \"\"\")\n","\n","    # 3. experiment_logs: í•™ìŠµ ë¡œê·¸ (ì—í­ë³„)\n","    cursor.execute(\"\"\"\n","    CREATE TABLE IF NOT EXISTS experiment_logs (\n","        id INTEGER PRIMARY KEY AUTOINCREMENT,\n","        experiment_id INTEGER,\n","        fold INTEGER,\n","        epoch INTEGER,\n","        train_loss REAL,\n","        val_loss REAL,\n","        train_acc REAL,\n","        val_acc REAL,\n","        FOREIGN KEY (experiment_id) REFERENCES experiments (id)\n","    )\n","    \"\"\")\n","\n","    # 4. experiment_tests: í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ê¸°ë¡\n","    cursor.execute(\"\"\"\n","    CREATE TABLE IF NOT EXISTS experiment_tests (\n","        id INTEGER PRIMARY KEY AUTOINCREMENT,\n","        experiment_id INTEGER,\n","        fold INTEGER,\n","        is_best_fold INTEGER DEFAULT 0,\n","        is_ensemble INTEGER DEFAULT 0,\n","        test_acc REAL,\n","        test_precision REAL,\n","        test_recall REAL,\n","        test_f1 REAL,\n","        conf_matrix_path TEXT,\n","        FOREIGN KEY (experiment_id) REFERENCES experiments (id)\n","    )\n","    \"\"\")\n","\n","    conn.commit()\n","    conn.close()\n","    print(\"âœ… SQLite DB ë° í…Œì´ë¸” ìƒì„± ì™„ë£Œ\")\n","\n","\n","def insert_experiment_with_folds(db_path, model_name, total_epochs, k_fold, lr, patience, fold_data_sizes):\n","    \"\"\"\n","    fold_data_sizes: List of (train_size, val_size) for each fold\n","    \"\"\"\n","    conn = sqlite3.connect(db_path)\n","    cursor = conn.cursor()\n","\n","    created_at = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","\n","    # 1. experiments í…Œì´ë¸”ì— ë“±ë¡\n","    cursor.execute(\"\"\"\n","        INSERT INTO experiments (\n","            model_name, total_epochs, k_fold, lr, patience, created_at\n","        ) VALUES (?, ?, ?, ?, ?, ?)\n","    \"\"\", (model_name, total_epochs, k_fold, lr, patience, created_at))\n","\n","    experiment_id = cursor.lastrowid\n","\n","    # 2. experiment_folds í…Œì´ë¸”ì— kê°œ fold ë“±ë¡\n","    fold_id_list = []\n","\n","    for i, (train_size, val_size) in enumerate(fold_data_sizes):\n","        cursor.execute(\"\"\"\n","            INSERT INTO experiment_folds (\n","                experiment_id, fold, train_size, val_size\n","            ) VALUES (?, ?, ?, ?)\n","        \"\"\", (experiment_id, i+1, train_size, val_size))\n","        fold_id_list.append(cursor.lastrowid)\n","\n","    conn.commit()\n","    conn.close()\n","\n","    print(f\"âœ… ì‹¤í—˜ ë“±ë¡ ì™„ë£Œ (experiment_id: {experiment_id})\")\n","    return experiment_id, fold_id_list\n","\n","\n","def insert_epoch_log(db_path, experiment_id, fold, epoch, train_loss, val_loss, train_acc, val_acc):\n","    conn = sqlite3.connect(db_path)\n","    cursor = conn.cursor()\n","\n","    cursor.execute(\"\"\"\n","        INSERT INTO experiment_logs (\n","            experiment_id, fold, epoch, train_loss, val_loss, train_acc, val_acc\n","        ) VALUES (?, ?, ?, ?, ?, ?, ?)\n","    \"\"\", (experiment_id, fold, epoch, train_loss, val_loss, train_acc, val_acc))\n","\n","    conn.commit()\n","    conn.close()\n","\n","\n","def update_fold_info(db_path, experiment_id, fold, best_model_path=None, early_stopped_epoch=None, log_plot_path=None):\n","    conn = sqlite3.connect(db_path)\n","    cursor = conn.cursor()\n","\n","    if best_model_path:\n","        cursor.execute(\"\"\"\n","            UPDATE experiment_folds\n","            SET best_model_path = ?\n","            WHERE experiment_id = ? AND fold = ?\n","        \"\"\", (best_model_path, experiment_id, fold))\n","\n","    if early_stopped_epoch:\n","        cursor.execute(\"\"\"\n","            UPDATE experiment_folds\n","            SET early_stopped_epoch = ?\n","            WHERE experiment_id = ? AND fold = ?\n","        \"\"\", (early_stopped_epoch, experiment_id, fold))\n","\n","    if log_plot_path:\n","        cursor.execute(\"\"\"\n","            UPDATE experiment_folds\n","            SET log_plot_path = ?\n","            WHERE experiment_id = ? AND fold = ?\n","        \"\"\", (log_plot_path, experiment_id, fold))\n","\n","    conn.commit()\n","    conn.close()\n","\n","\n","def insert_test_result(\n","    db_path,\n","    experiment_id,\n","    fold,\n","    is_best_fold,\n","    is_ensemble,\n","    test_acc,\n","    test_precision,\n","    test_recall,\n","    test_f1,\n","    conf_matrix_path\n","):\n","    conn = sqlite3.connect(db_path)\n","    cursor = conn.cursor()\n","\n","    cursor.execute(\"\"\"\n","        INSERT INTO experiment_tests (\n","            experiment_id, fold, is_best_fold, is_ensemble,\n","            test_acc, test_precision, test_recall, test_f1, conf_matrix_path\n","        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n","    \"\"\", (\n","        experiment_id, fold, is_best_fold, is_ensemble,\n","        test_acc, test_precision, test_recall, test_f1, conf_matrix_path\n","    ))\n","\n","    conn.commit()\n","    conn.close()\n","\n","\n","def plot_log_from_db(db_path, experiment_id, fold, model_name, save_path):\n","    conn = sqlite3.connect(db_path)\n","\n","    # ë¡œê·¸ ì¡°íšŒ\n","    df = pd.read_sql_query(\"\"\"\n","        SELECT epoch, train_loss, val_loss, train_acc, val_acc\n","        FROM experiment_logs\n","        WHERE experiment_id = ? AND fold = ?\n","        ORDER BY epoch\n","    \"\"\", conn, params=(experiment_id, fold))\n","\n","    conn.close()\n","\n","    if df.empty:\n","        print(f\"âŒ ë¡œê·¸ ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (experiment_id={experiment_id}, fold={fold})\")\n","        return\n","\n","    # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°\n","    plt.figure(figsize=(12, 5))\n","\n","    # Loss ê·¸ë˜í”„\n","    plt.subplot(1, 2, 1)\n","    plt.plot(df['epoch'], df['train_loss'], label='Train Loss', marker='o')\n","    plt.plot(df['epoch'], df['val_loss'], label='Val Loss', marker='s')\n","    plt.title(f\"Loss Curve - {model_name} (Fold {fold})\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend()\n","    plt.grid(True)\n","\n","    # Accuracy ê·¸ë˜í”„\n","    plt.subplot(1, 2, 2)\n","    plt.plot(df['epoch'], df['train_acc'], label='Train Acc', marker='o')\n","    plt.plot(df['epoch'], df['val_acc'], label='Val Acc', marker='s')\n","    plt.title(f\"Accuracy Curve - {model_name} (Fold {fold})\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Accuracy\")\n","    plt.legend()\n","    plt.grid(True)\n","\n","    plt.tight_layout()\n","    plt.savefig(save_path)\n","    plt.close()\n","    print(f\"ğŸ“ˆ Fold {fold} í•™ìŠµ ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {save_path}\")\n","\n","\n","def get_best_fold_path(db_path, experiment_id):\n","    import sqlite3\n","    conn = sqlite3.connect(db_path)\n","    cursor = conn.cursor()\n","\n","    # 1. ê° foldë³„ í‰ê·  val_loss ê³„ì‚°\n","    cursor.execute(\"\"\"\n","        SELECT fold, AVG(val_loss) as avg_val_loss\n","        FROM experiment_logs\n","        WHERE experiment_id = ?\n","        GROUP BY fold\n","        ORDER BY avg_val_loss ASC\n","        LIMIT 1\n","    \"\"\", (experiment_id,))\n","    result = cursor.fetchone()\n","\n","    if not result:\n","        return None, None\n","\n","    best_fold = result[0]\n","\n","    # 2. best_model_path ì¡°íšŒ\n","    cursor.execute(\"\"\"\n","        SELECT best_model_path FROM experiment_folds\n","        WHERE experiment_id = ? AND fold = ?\n","    \"\"\", (experiment_id, best_fold))\n","    model_path_result = cursor.fetchone()\n","\n","    conn.close()\n","\n","    if model_path_result:\n","        return best_fold, model_path_result[0]\n","    else:\n","        return best_fold, None\n"],"metadata":{"id":"cpYmEuMD1UBA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cross Validation K-Fold ì „ìš© dataloader\n","\n","import torch\n","\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader, random_split\n","\n","def set_dataloader():\n","    # ë¼ë²¨ ëª©ë¡\n","    labels = os.listdir(root_dir)\n","    label_to_index = {label: idx for idx, label in enumerate(labels)}\n","    num_classes = len(labels)\n","\n","    # ì „ì²´ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ ìƒì„± (í´ë”ë³„ë¡œ ì¼ì • ë¹„ìœ¨ ìœ ì§€)\n","    data_list = {label: [] for label in labels}\n","    for label in labels:\n","        data_dir = os.path.join(root_dir, label)\n","        datas = os.listdir(data_dir)\n","        for img_name in datas:\n","            img_path = os.path.join(data_dir, img_name)\n","            data_list[label].append((img_path, label_to_index[label]))\n","\n","    # ë°ì´í„°ì…‹ ë¶„í•  (í´ë”ë³„ë¡œ ìœ ì§€)\n","    train_data, test_data = [], []\n","    for label, items in data_list.items():\n","        random.shuffle(items)\n","        total_size = len(items)\n","\n","        train_data.extend(items[:6])\n","        test_data.extend(items[6:])\n","\n","    # ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜\n","    class CustomImageDataset(Dataset):\n","        def __init__(self, data, transform=None):\n","            self.data = data\n","            self.transform = transform\n","\n","        def __len__(self):\n","            return len(self.data)\n","\n","        def __getitem__(self, idx):\n","            img_path, label = self.data[idx]\n","            img = cv2.imread(img_path)\n","            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","            if self.transform:\n","                img = self.transform(img)\n","\n","            return img, label\n","\n","    # ë°ì´í„° ë³€í™˜ ì •ì˜\n","    transform = transforms.Compose([\n","        transforms.ToPILImage(),\n","        transforms.Resize((224, 224)),  # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ])\n","\n","    # ë°ì´í„°ì…‹ ìƒì„±\n","    train_dataset = CustomImageDataset(train_data, transform=transform)\n","    test_dataset = CustomImageDataset(test_data, transform=transform)\n","\n","    batch_size = 4\n","\n","    # ë°ì´í„°ë¡œë” ìƒì„±\n","    dataloader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n","    dataloader_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n","\n","    # ë°ì´í„°ì…‹ í¬ê¸° ì¶œë ¥\n","    print(f\"Training Dataset Size: {len(train_dataset)}\")\n","    print(f\"Test Dataset Size: {len(test_dataset)}\")\n","\n","    # ë°ì´í„° í™•ì¸\n","    for images, labels in dataloader_train:\n","        print(f\"Train Batch Image Shape: {images.shape}\")\n","        print(f\"Train Batch Labels: {labels}\")\n","        break\n","\n","    return train_dataset, dataloader_train, dataloader_test, num_classes"],"metadata":{"id":"KrR6Gu4DzOvO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cross Validation K-Fold í•™ìŠµ ì½”ë“œ\n","\n","from sklearn.model_selection import StratifiedKFold\n","from tqdm import tqdm\n","import torch.optim as optim\n","import os\n","import torch.nn as nn\n","import torch\n","\n","def train_model_cv_kfold(\n","    model_name, dataset, num_classes, save_dir, device,\n","    k=5, epochs=10, lr=0.0001, patience=3,\n","    db_path=\"/content/drive/MyDrive/KTB/personal_mission/experiment_log.db\"\n","):\n","    # Stratified K-Fold ì¤€ë¹„\n","    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n","    targets = [label for _, label in dataset]  # labelë§Œ ì¶”ì¶œ\n","\n","    # foldë³„ ë°ì´í„° ì‚¬ì´ì¦ˆ íŒŒì•…\n","    fold_data_sizes = []\n","    fold_indices = []\n","    for train_idx, val_idx in skf.split(dataset, targets):\n","        fold_data_sizes.append((len(train_idx), len(val_idx)))\n","        fold_indices.append((train_idx, val_idx))\n","\n","    # âœ… ì‹¤í—˜ ë“±ë¡ ë° fold ID ì´ˆê¸°í™”\n","    experiment_id, _ = insert_experiment_with_folds(\n","        db_path=db_path,\n","        model_name=model_name,\n","        total_epochs=epochs,\n","        k_fold=k,\n","        lr=lr,\n","        patience=patience,\n","        fold_data_sizes=fold_data_sizes\n","    )\n","\n","    # foldë³„ ëª¨ë¸, ì˜µí‹°ë§ˆì´ì €, ë°ì´í„°ë¡œë” ì´ˆê¸°í™”\n","    fold_models, fold_optimizers = [], []\n","    fold_train_loaders, fold_val_loaders = [], []\n","\n","    for i, (train_idx, val_idx) in enumerate(fold_indices):\n","        train_subset = torch.utils.data.Subset(dataset, train_idx)\n","        val_subset = torch.utils.data.Subset(dataset, val_idx)\n","\n","        print(f\"ğŸ“Š Fold {i+1} - Train Size: {len(train_subset)}, Validation Size: {len(val_idx)}\")\n","\n","        model = get_model(model_name, num_classes).to(device)\n","        optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","        train_loader = torch.utils.data.DataLoader(train_subset, batch_size=4, shuffle=True)\n","        val_loader = torch.utils.data.DataLoader(val_subset, batch_size=4, shuffle=False)\n","\n","        fold_models.append(model)\n","        fold_optimizers.append(optimizer)\n","        fold_train_loaders.append(train_loader)\n","        fold_val_loaders.append(val_loader)\n","\n","    # í•™ìŠµ ë£¨í”„\n","    best_avg_val_loss = float(\"inf\")\n","    no_improve_count = 0\n","\n","    for epoch in tqdm(range(epochs), desc=f\"[{model_name} CV-KFold]\"):\n","        fold_val_losses, fold_val_accs = [], []\n","\n","        for fold in range(k):\n","            model = fold_models[fold]\n","            optimizer = fold_optimizers[fold]\n","            train_loader = fold_train_loaders[fold]\n","            val_loader = fold_val_loaders[fold]\n","\n","            # í•™ìŠµ\n","            train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, device)\n","\n","            # ê²€ì¦\n","            val_loss, val_acc = validate_model(model, val_loader, device, nn.CrossEntropyLoss())\n","\n","            # âœ… ë¡œê·¸ ì €ì¥\n","            insert_epoch_log(db_path, experiment_id, fold+1, epoch+1, train_loss, val_loss, train_acc, val_acc)\n","\n","            fold_val_losses.append(val_loss)\n","            fold_val_accs.append(val_acc)\n","\n","        # Cross-Fold í‰ê· ìœ¼ë¡œ early stop ê°ì‹œ\n","        avg_val_loss = sum(fold_val_losses) / k\n","\n","        if avg_val_loss < best_avg_val_loss:\n","            best_avg_val_loss = avg_val_loss\n","            no_improve_count = 0\n","\n","            # âœ… foldë³„ ëª¨ë¸ ì €ì¥\n","            for fold in range(k):\n","                model_path = os.path.join(save_dir, f\"best_model_{model_name}_fold{fold+1}.pth\")\n","                torch.save(fold_models[fold].state_dict(), model_path)\n","\n","        else:\n","            no_improve_count += 1\n","\n","        if no_improve_count >= patience:\n","            print(f\"ğŸ›‘ Early stopping at epoch {epoch+1}\")\n","            break\n","\n","    # âœ… foldë³„ í•™ìŠµ ì¢…ë£Œ í›„ ì •ë³´ ì—…ë°ì´íŠ¸\n","    for fold in range(k):\n","        model_path = os.path.join(save_dir, f\"best_model_{model_name}_fold{fold+1}.pth\")\n","        log_plot_path = os.path.join(save_dir, f\"training_plot_{model_name}_fold{fold+1}.png\")\n","\n","        # ê·¸ë˜í”„ ì €ì¥\n","        plot_log_from_db(\n","            db_path=db_path,\n","            experiment_id=experiment_id,\n","            fold=fold + 1,  # âœ… ìˆ«ìí˜• fold index\n","            model_name=model_name,\n","            save_path=log_plot_path\n","        )\n","\n","        # fold ì •ë³´ ì—…ë°ì´íŠ¸\n","        update_fold_info(\n","            db_path,\n","            experiment_id,\n","            fold=fold+1,\n","            best_model_path=model_path,\n","            early_stopped_epoch=epoch+1,\n","            log_plot_path=log_plot_path\n","        )\n","\n","    print(f\"\\nâœ… {model_name} ëª¨ë¸ì˜ CV K-Fold í•™ìŠµ ì™„ë£Œ (experiment_id: {experiment_id})\")\n","    return experiment_id\n","\n","def train_one_epoch(model, dataloader, optimizer, device):\n","    model.train()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    criterion = nn.CrossEntropyLoss()\n","\n","    for images, labels in dataloader:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        _, predicted = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    avg_loss = running_loss / len(dataloader)\n","    accuracy = correct / total\n","    return avg_loss, accuracy\n","\n","def validate_model(model, dataloader, device, criterion):\n","    model.eval()\n","    total_loss = 0.0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for images, labels in dataloader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item()\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    avg_loss = total_loss / len(dataloader)\n","    accuracy = correct / total\n","    return avg_loss, accuracy"],"metadata":{"id":"pMI7Vbbe0CIx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# í‰ê°€ í•¨ìˆ˜\n","\n","import seaborn as sns\n","import torch.nn as nn\n","import torchvision.models as models\n","\n","from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n","\n","# Fine-tuned ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜\n","def load_fine_tuned_model(model_name, num_classes, checkpoint_path, device):\n","    try:\n","        if model_name == 'resnet50':\n","            model = models.resnet50(pretrained=True)\n","            model.fc = nn.Linear(model.fc.in_features, num_classes)\n","        elif model_name == 'resnet152':\n","            model = models.resnet152(pretrained=True)\n","            model.fc = nn.Linear(model.fc.in_features, num_classes)\n","        elif model_name == 'vgg16_bn':\n","            model = models.vgg16_bn(pretrained=True)\n","            model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n","        elif model_name == 'densenet201':\n","            model = models.densenet201(pretrained=True)\n","            model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n","        elif model_name == 'seresnext101':\n","            model = models.resnext101_32x8d(pretrained=True)  # SE-ResNeXtëŠ” torchvisionì— ì—†ìŒ â†’ ìœ ì‚¬í•œ êµ¬ì¡°ë¡œ ëŒ€ì²´\n","            model.fc = nn.Linear(model.fc.in_features, num_classes)\n","        elif model_name == 'convnext_base':\n","            model = models.convnext_base(pretrained=True)\n","            model.classifier[2] = nn.Linear(model.classifier[2].in_features, num_classes)\n","\n","        model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n","        model.to(device)\n","        return model\n","\n","    except Exception as e:\n","        print(f\"âš ï¸ ëª¨ë¸ {model_name} ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n","        return None  # ì˜¤ë¥˜ ë°œìƒ ì‹œ None ë°˜í™˜\n","\n","# Confusion Matrix ì‹œê°í™” ë° ì €ì¥\n","def plot_confusion_matrix(y_true, y_pred, classes, model_name, save_path):\n","    cm = confusion_matrix(y_true, y_pred)\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n","    plt.xlabel('Predicted Label')\n","    plt.ylabel('True Label')\n","    plt.title(f'Confusion Matrix ({model_name})')\n","    plt.savefig(save_path)\n","    plt.close()\n","\n","def evaluate_model_and_log(\n","    experiment_id,\n","    model_name,\n","    test_loader,\n","    num_classes,\n","    device,\n","    save_dir,\n","    fold=None,\n","    is_best_fold=0,\n","    is_ensemble=0,\n","    db_path=\"/content/drive/MyDrive/KTB/personal_mission/experiment_log.db\",\n","    class_dir='almond/dataset'\n","):\n","    try:\n","        best_fold, model_path = get_best_fold_path(db_path, experiment_id)\n","        fold = best_fold\n","\n","        # ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n","        model = load_fine_tuned_model(model_name, num_classes, model_path, device)\n","        if model is None:\n","            print(f\"âŒ ëª¨ë¸ {model_path} ë¡œë“œ ì‹¤íŒ¨\")\n","            return\n","\n","        model.eval()\n","        all_preds, all_labels = [], []\n","\n","        with torch.no_grad():\n","            for images, labels in test_loader:\n","                images, labels = images.to(device), labels.to(device)\n","                outputs = model(images)\n","                _, preds = torch.max(outputs, 1)\n","                all_preds.extend(preds.cpu().numpy())\n","                all_labels.extend(labels.cpu().numpy())\n","\n","        # í´ë˜ìŠ¤ëª… (Confusion Matrixìš©)\n","        class_names = os.listdir(class_dir)\n","\n","        # ì§€í‘œ ê³„ì‚°\n","        acc = accuracy_score(all_labels, all_preds)\n","        precision = precision_score(all_labels, all_preds, average='weighted')\n","        recall = recall_score(all_labels, all_preds, average='weighted')\n","        f1 = f1_score(all_labels, all_preds, average='weighted')\n","\n","        print(f\"âœ… í‰ê°€ ì™„ë£Œ - Acc: {acc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n","\n","        # Confusion Matrix ì‹œê°í™”\n","        cm = confusion_matrix(all_labels, all_preds)\n","        plt.figure(figsize=(8, 6))\n","        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n","        plt.xlabel('Predicted Label')\n","        plt.ylabel('True Label')\n","        plt.title(f'Confusion Matrix ({model_name})')\n","\n","        # ì €ì¥ ê²½ë¡œ ìƒì„± ë° ì €ì¥\n","        os.makedirs(save_dir, exist_ok=True)\n","        suffix = \"ensemble\" if is_ensemble else (f\"best_fold{fold}\" if is_best_fold else f\"fold{fold}\")\n","        cm_path = os.path.join(save_dir, f\"conf_matrix_{model_name}_{suffix}.png\")\n","        plt.savefig(cm_path)\n","        plt.close()\n","\n","        # âœ… DB ì €ì¥\n","        insert_test_result(\n","            db_path=db_path,\n","            experiment_id=experiment_id,\n","            fold=fold if fold is not None else 0,\n","            is_best_fold=is_best_fold,\n","            is_ensemble=is_ensemble,\n","            test_acc=acc,\n","            test_precision=precision,\n","            test_recall=recall,\n","            test_f1=f1,\n","            conf_matrix_path=cm_path\n","        )\n","\n","    except Exception as e:\n","        print(f\"âŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n","\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# ê° fold í‰ê°€\n","def evaluate_fold_models(\n","    experiment_id,\n","    model_name,\n","    k,\n","    save_dir,\n","    test_loader,\n","    num_classes,\n","    device,\n","    db_path=\"/content/drive/MyDrive/KTB/personal_mission/experiment_log.db\"\n","):\n","    best_acc = 0\n","    best_fold = None\n","\n","    for fold in range(1, k+1):  # fold 1~5\n","        model_path = os.path.join(save_dir, f\"best_model_{model_name}_fold{fold}.pth\")\n","        model = load_fine_tuned_model(model_name, num_classes, model_path, device)\n","        if model is None:\n","            continue\n","\n","        model.eval()\n","        all_preds, all_labels = [], []\n","\n","        with torch.no_grad():\n","            for images, labels in test_loader:\n","                images, labels = images.to(device), labels.to(device)\n","                outputs = model(images)\n","                _, preds = torch.max(outputs, 1)\n","                all_preds.extend(preds.cpu().numpy())\n","                all_labels.extend(labels.cpu().numpy())\n","\n","        acc = accuracy_score(all_labels, all_preds)\n","        precision = precision_score(all_labels, all_preds, average='weighted')\n","        recall = recall_score(all_labels, all_preds, average='weighted')\n","        f1 = f1_score(all_labels, all_preds, average='weighted')\n","\n","        if acc > best_acc:\n","            best_acc = acc\n","            best_fold = fold\n","\n","        # í´ë˜ìŠ¤ëª… (Confusion Matrixìš©)\n","        class_dir='almond/dataset'\n","        class_names = os.listdir(class_dir)\n","\n","        # Confusion matrix ì €ì¥\n","        cm = confusion_matrix(all_labels, all_preds)\n","        plt.figure(figsize=(8, 6))\n","        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n","        plt.title(f'Confusion Matrix - Fold {fold}')\n","        plt.xlabel('Predicted Label')\n","        plt.ylabel('True Label')\n","\n","        cm_path = os.path.join(save_dir, f\"conf_matrix_{model_name}_fold{fold}.png\")\n","        plt.savefig(cm_path)\n","        plt.close()\n","\n","        # âœ… DB ì €ì¥\n","        insert_test_result(\n","            db_path=db_path,\n","            experiment_id=experiment_id,\n","            fold=fold,\n","            is_best_fold=0,\n","            is_ensemble=0,\n","            test_acc=acc,\n","            test_precision=precision,\n","            test_recall=recall,\n","            test_f1=f1,\n","            conf_matrix_path=cm_path\n","        )\n","\n","    return best_fold\n","\n","# Val loss ê¸°ì¤€ best fold ëª¨ë¸ í‰ê°€\n","def evaluate_best_fold_model(\n","    experiment_id,\n","    model_name,\n","    save_dir,\n","    test_loader,\n","    num_classes,\n","    device,\n","    db_path=\"/content/drive/MyDrive/KTB/personal_mission/experiment_log.db\"\n","):\n","    best_fold, model_path = get_best_fold_path(db_path, experiment_id)\n","    model = load_fine_tuned_model(model_name, num_classes, model_path, device)\n","    if model is None:\n","        return\n","\n","    model.eval()\n","    all_preds, all_labels = [], []\n","\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, preds = torch.max(outputs, 1)\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    acc = accuracy_score(all_labels, all_preds)\n","    precision = precision_score(all_labels, all_preds, average='weighted')\n","    recall = recall_score(all_labels, all_preds, average='weighted')\n","    f1 = f1_score(all_labels, all_preds, average='weighted')\n","\n","    # í´ë˜ìŠ¤ëª… (Confusion Matrixìš©)\n","    class_dir='almond/dataset'\n","    class_names = os.listdir(class_dir)\n","\n","    # Confusion matrix ì €ì¥\n","    cm = confusion_matrix(all_labels, all_preds)\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n","    plt.title(f'Best Fold Confusion Matrix')\n","    plt.xlabel('Predicted Label')\n","    plt.ylabel('True Label')\n","\n","\n","    cm_path = os.path.join(save_dir, f\"conf_matrix_{model_name}_best_fold{best_fold}.png\")\n","    plt.savefig(cm_path)\n","    plt.close()\n","\n","    # âœ… DB ì €ì¥\n","    insert_test_result(\n","        db_path=db_path,\n","        experiment_id=experiment_id,\n","        fold=best_fold,\n","        is_best_fold=1,\n","        is_ensemble=0,\n","        test_acc=acc,\n","        test_precision=precision,\n","        test_recall=recall,\n","        test_f1=f1,\n","        conf_matrix_path=cm_path\n","    )\n","\n","import numpy as np\n","\n","# ì•™ìƒë¸” ëª¨ë¸ í‰ê°€\n","def evaluate_ensemble_model(\n","    experiment_id,\n","    model_name,\n","    k,\n","    save_dir,\n","    test_loader,\n","    num_classes,\n","    device,\n","    db_path=\"/content/drive/MyDrive/KTB/personal_mission/experiment_log.db\"\n","):\n","    models_list = []\n","    for fold in range(1, k+1):\n","        model_path = os.path.join(save_dir, f\"best_model_{model_name}_fold{fold}.pth\")\n","        model = load_fine_tuned_model(model_name, num_classes, model_path, device)\n","        if model:\n","            model.eval()\n","            models_list.append(model)\n","\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images = images.to(device)\n","            labels = labels.cpu().numpy()\n","            outputs_sum = None\n","\n","            for model in models_list:\n","                outputs = model(images)\n","                probs = torch.softmax(outputs, dim=1).cpu().numpy()\n","                outputs_sum = probs if outputs_sum is None else outputs_sum + probs\n","\n","            avg_probs = outputs_sum / len(models_list)\n","            preds = np.argmax(avg_probs, axis=1)\n","\n","            all_preds.extend(preds)\n","            all_labels.extend(labels)\n","\n","    acc = accuracy_score(all_labels, all_preds)\n","    precision = precision_score(all_labels, all_preds, average='weighted')\n","    recall = recall_score(all_labels, all_preds, average='weighted')\n","    f1 = f1_score(all_labels, all_preds, average='weighted')\n","\n","    # í´ë˜ìŠ¤ëª… (Confusion Matrixìš©)\n","    class_dir='almond/dataset'\n","    class_names = os.listdir(class_dir)\n","\n","    # Confusion matrix ì €ì¥\n","    cm = confusion_matrix(all_labels, all_preds)\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n","    plt.title(f'Ensemble Confusion Matrix')\n","    plt.xlabel('Predicted Label')\n","    plt.ylabel('True Label')\n","\n","    cm_path = os.path.join(save_dir, f\"conf_matrix_{model_name}_ensemble.png\")\n","    plt.savefig(cm_path)\n","    plt.close()\n","\n","    # âœ… DB ì €ì¥\n","    insert_test_result(\n","        db_path=db_path,\n","        experiment_id=experiment_id,\n","        fold=0,  # ì•™ìƒë¸”ì€ fold ì—†ìŒ\n","        is_best_fold=0,\n","        is_ensemble=1,\n","        test_acc=acc,\n","        test_precision=precision,\n","        test_recall=recall,\n","        test_f1=f1,\n","        conf_matrix_path=cm_path\n","    )\n","\n"],"metadata":{"id":"gKC286ZS0TqS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fine-tuning\n","dataloader_train, dataloader_val, dataloader_test, num_classes = set_dataloader()\n","\n","save_dir = create_next_directory('/content/drive/MyDrive/KTB/personal_mission/train_dir')\n","for model_name in ['resnet50', 'resnet152', 'vgg16_bn', 'densenet201', 'seresnext101', 'convnext_base']:\n","    print(f\"Training {model_name}...\")\n","    train_model(model_name, dataloader_train, dataloader_val, num_classes, save_dir, device, epochs=200, lr=0.00005, patience=10)\n","process_single_experiment(dataloader_test, device, num_classes, save_dir)\n","\n","\n","# Augmentation + Fine-tuning\n","dataloader_train, dataloader_val, dataloader_test, num_classes = set_aug_dataloader()\n","\n","save_dir = create_next_directory('/content/drive/MyDrive/KTB/personal_mission/aug_train_dir')\n","for model_name in ['resnet50', 'resnet152', 'vgg16_bn', 'densenet201', 'seresnext101', 'convnext_base']:\n","    print(f\"Training {model_name}...\")\n","    train_model(model_name, dataloader_train, dataloader_val, num_classes, save_dir, device, epochs=200, lr=0.00005, patience=10)\n","process_single_experiment(dataloader_test, device, num_classes, save_dir)\n","\n","# C-V K-Fold + Fine-tuning\n","train_dataset, dataloader_train, dataloader_test, num_classes = set_dataloader()\n","\n","save_dir = create_next_directory('/content/drive/MyDrive/KTB/personal_mission/k-fold_aug_train_dir')\n","for model_name in ['resnet50', 'resnet152', 'vgg16_bn', 'densenet201', 'seresnext101', 'convnext_base']:\n","    print(f\"Training {model_name}...\")\n","    experiment_id =  train_model_cv_kfold(model_name, train_dataset, num_classes, save_dir, device, k=3, epochs=200, lr=0.00005, patience=10)\n","    print(f\"Testing {model_name}...\")\n","    evaluate_model_and_log(experiment_id, model_name, dataloader_test, num_classes, device, save_dir)\n","    evaluate_fold_models(experiment_id, model_name, 3, save_dir, dataloader_test, num_classes, device, db_path=\"/content/drive/MyDrive/KTB/personal_mission/experiment_log.db\")\n","    evaluate_best_fold_model(experiment_id, model_name, save_dir, dataloader_test, num_classes, device, db_path=\"/content/drive/MyDrive/KTB/personal_mission/experiment_log.db\")\n","    evaluate_ensemble_model(experiment_id, model_name, 3, save_dir, dataloader_test, num_classes, device, db_path=\"/content/drive/MyDrive/KTB/personal_mission/experiment_log.db\")"],"metadata":{"id":"Ly5fTtFE0V-S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Data Augmentation + Cross Validation K-Fold**"],"metadata":{"id":"ctaPZeF82HRD"}},{"cell_type":"code","source":["# Augmentation í¬í•¨ Dataset í´ë˜ìŠ¤\n","\n","from PIL import Image\n","from torchvision.transforms.functional import rotate\n","\n","class RotatedAugmentedDataset(Dataset):\n","    def __init__(self, original_dataset, indices):\n","        self.original_dataset = original_dataset\n","        self.indices = indices\n","        self.angles = [0, 90, 180, 270]\n","\n","    def __len__(self):\n","        return len(self.indices) * 4\n","\n","    def __getitem__(self, idx):\n","        img_idx = idx // 4\n","        angle_idx = idx % 4\n","\n","        image = self.original_dataset[img_idx][0]\n","        label = self.original_dataset[img_idx][1]\n","\n","        rotated_image = rotate(image, self.angles[idx % 4])\n","\n","        return rotated_image, label"],"metadata":{"id":"cPJGlU7N2Ml2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Augmentation + Cross Validation K-Fold í•™ìŠµ ì½”ë“œ\n","\n","from sklearn.model_selection import StratifiedKFold\n","from tqdm import tqdm\n","import torch.optim as optim\n","import os\n","import torch.nn as nn\n","import torch\n","\n","def train_model_cv_kfold(\n","    model_name, dataset, num_classes, save_dir, device,\n","    k=5, epochs=10, lr=0.0001, patience=3,\n","    db_path=\"/content/drive/MyDrive/KTB/personal_mission/experiment_log.db\"\n","):\n","    # Stratified K-Fold ì¤€ë¹„\n","    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n","    targets = [label for _, label in dataset]  # labelë§Œ ì¶”ì¶œ\n","\n","    # foldë³„ ë°ì´í„° ì‚¬ì´ì¦ˆ íŒŒì•…\n","    fold_data_sizes = []\n","    fold_indices = []\n","    for train_idx, val_idx in skf.split(dataset, targets):\n","        fold_data_sizes.append((len(train_idx), len(val_idx)))\n","        fold_indices.append((train_idx, val_idx))\n","\n","    # âœ… ì‹¤í—˜ ë“±ë¡ ë° fold ID ì´ˆê¸°í™”\n","    experiment_id, _ = insert_experiment_with_folds(\n","        db_path=db_path,\n","        model_name=model_name,\n","        total_epochs=epochs,\n","        k_fold=k,\n","        lr=lr,\n","        patience=patience,\n","        fold_data_sizes=fold_data_sizes\n","    )\n","\n","    # foldë³„ ëª¨ë¸, ì˜µí‹°ë§ˆì´ì €, ë°ì´í„°ë¡œë” ì´ˆê¸°í™”\n","    fold_models, fold_optimizers = [], []\n","    fold_train_loaders, fold_val_loaders = [], []\n","\n","    for i, (train_idx, val_idx) in enumerate(fold_indices):\n","        augmented_train_dataset = RotatedAugmentedDataset(dataset, train_idx)\n","        val_subset = torch.utils.data.Subset(dataset, val_idx)\n","\n","        print(f\"ğŸ“Š Fold {i+1} - Augmented Train Dataset Size: {len(augmented_train_dataset)} (Original: {len(train_idx)}), Validation Size: {len(val_idx)}\")\n","\n","        show_augmented_images(augmented_train_dataset)\n","\n","        model = get_model(model_name, num_classes).to(device)\n","        optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","        train_loader = torch.utils.data.DataLoader(augmented_train_dataset, batch_size=4, shuffle=True)\n","        val_loader = torch.utils.data.DataLoader(val_subset, batch_size=4, shuffle=False)\n","\n","        fold_models.append(model)\n","        fold_optimizers.append(optimizer)\n","        fold_train_loaders.append(train_loader)\n","        fold_val_loaders.append(val_loader)\n","\n","    # í•™ìŠµ ë£¨í”„\n","    best_avg_val_loss = float(\"inf\")\n","    no_improve_count = 0\n","\n","    for epoch in tqdm(range(epochs), desc=f\"[{model_name} CV-KFold]\"):\n","        fold_val_losses, fold_val_accs = [], []\n","\n","        for fold in range(k):\n","            model = fold_models[fold]\n","            optimizer = fold_optimizers[fold]\n","            train_loader = fold_train_loaders[fold]\n","            val_loader = fold_val_loaders[fold]\n","\n","            # í•™ìŠµ\n","            train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, device)\n","\n","            # ê²€ì¦\n","            val_loss, val_acc = validate_model(model, val_loader, device, nn.CrossEntropyLoss())\n","\n","            # âœ… ë¡œê·¸ ì €ì¥\n","            insert_epoch_log(db_path, experiment_id, fold+1, epoch+1, train_loss, val_loss, train_acc, val_acc)\n","\n","            fold_val_losses.append(val_loss)\n","            fold_val_accs.append(val_acc)\n","\n","        # Cross-Fold í‰ê· ìœ¼ë¡œ early stop ê°ì‹œ\n","        avg_val_loss = sum(fold_val_losses) / k\n","\n","        if avg_val_loss < best_avg_val_loss:\n","            best_avg_val_loss = avg_val_loss\n","            no_improve_count = 0\n","\n","            # âœ… foldë³„ ëª¨ë¸ ì €ì¥\n","            for fold in range(k):\n","                model_path = os.path.join(save_dir, f\"best_model_{model_name}_fold{fold+1}.pth\")\n","                torch.save(fold_models[fold].state_dict(), model_path)\n","\n","        else:\n","            no_improve_count += 1\n","\n","        if no_improve_count >= patience:\n","            print(f\"ğŸ›‘ Early stopping at epoch {epoch+1}\")\n","            break\n","\n","    # âœ… foldë³„ í•™ìŠµ ì¢…ë£Œ í›„ ì •ë³´ ì—…ë°ì´íŠ¸\n","    for fold in range(k):\n","        model_path = os.path.join(save_dir, f\"best_model_{model_name}_fold{fold+1}.pth\")\n","        log_plot_path = os.path.join(save_dir, f\"training_plot_{model_name}_fold{fold+1}.png\")\n","\n","        # ê·¸ë˜í”„ ì €ì¥\n","        plot_log_from_db(\n","            db_path=db_path,\n","            experiment_id=experiment_id,\n","            fold=fold + 1,  # âœ… ìˆ«ìí˜• fold index\n","            model_name=model_name,\n","            save_path=log_plot_path\n","        )\n","\n","        # fold ì •ë³´ ì—…ë°ì´íŠ¸\n","        update_fold_info(\n","            db_path,\n","            experiment_id,\n","            fold=fold+1,\n","            best_model_path=model_path,\n","            early_stopped_epoch=epoch+1,\n","            log_plot_path=log_plot_path\n","        )\n","\n","    print(f\"\\nâœ… {model_name} ëª¨ë¸ì˜ CV K-Fold í•™ìŠµ ì™„ë£Œ (experiment_id: {experiment_id})\")\n","    return experiment_id\n"],"metadata":{"id":"KBWgCGSG2R5J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Zu27wrX_2XDx"},"execution_count":null,"outputs":[]}]}